{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "# Dir setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Tools\n",
    "rabbit = \"/NAS/wg_looking/Tools/RabbitQC-0.0.1/rabbit_qc\"\n",
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "# minimap2 = \"/NAS/wg_looking/Tools/minimap2/minimap2\"\n",
    "minimap2 = \"/NAS/wg_looking/Tools/minimap2-2.22_x64-linux/minimap2\"\n",
    "bedtools = \"/NAS/wg_looking/Tools/bedtools2/bin/bedtools\"\n",
    "transClean = \"/NAS/wg_looking/Tools/TranscriptClean-master/TranscriptClean.py\"\n",
    "gatk = \"/NAS/wg_looking/Tools/gatk-4.2.0.0/gatk\"\n",
    "porechop = \"/NAS/wg_looking/Tools/Porechop/porechop-runner.py\"\n",
    "blastDir = \"/NAS/wg_looking/Tools/ncbi-blast-2.11.0+/bin\"\n",
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "cuteSV = \"/NAS/wg_looking/Tools/cuteSV-cuteSV-v1.0.12/src/cuteSV/cuteSV\"\n",
    "seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "\n",
    "\n",
    "\n",
    "# database\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "hg38_main = \"/NAS/wg_tkl/Human_ref/mainChr_hg38_UCSC.fa\"\n",
    "gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "\n",
    "\n",
    "# dir\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "### fastq files\n",
    "fastq_dict = {\n",
    "              'totalRNA_20210623': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              'sgRNA_20210624': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/fastq_pass/barcode24/BC24.fastq',\n",
    "              'sgRNA_20210801_BC14': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode14/BC14.fastq',\n",
    "              'sgRNA_20210801_BC15': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode15/BC15.fastq',\n",
    "              \"sgRNA_MDAMB231_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode16/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode17/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode18/basecall_pass.fastq\"\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "fastq_dict = {\n",
    "              # 'sgRNA_20210527': '/NAS/wg_looking/gRNA_ONT/data/no_sample/20210527_0710_X2_FAQ12460_0bbed906/KT_test/basecall_pass.fastq',\n",
    "              'sgRNA_20210617': '/NAS/wg_looking/gRNA_ONT/data/no_sample/20210617_0817_X2_FAQ14387_1b628e6b/basecall_pass.fastq',\n",
    "              'totalRNA_20210623': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              # 'sgRNA_20210624': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/fastq_pass/barcode24/BC24.fastq',\n",
    "              # 'totalRNA_20210715': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/20210715_1103_X2_FAQ42413_d69cb2d1/basecall_pass.fastq',\n",
    "              # 'sgRNA_20210801_BC14': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode14/BC14.fastq',\n",
    "              # 'sgRNA_20210801_BC15': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode15/BC15.fastq',\n",
    "              # 'sgRNA_20210802': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_02_K562_aluL1_18UMI_2/no_sample/20210802_0412_X2_FAQ42413_d2eae930/basecall_pass.fastq',\n",
    "              # 'totalRNA_20210803': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_03_k562_totalRNA_18UMI-1-2/no_sample/20210803_0341_X2_FAQ61633_75e6e880/fastq_pass/basecall_pass.fastq',\n",
    "              # \"totalRNA_20210904_BC13\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode13/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC14\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode14/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC15\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode15/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC16\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode16/basecalls.fastq\",\n",
    "              # \"totalRNA_20210907_BC17\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode17/basecalls.fastq\",\n",
    "              # \"totalRNA_20210907_BC18\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode18/basecalls.fastq\",\n",
    "              # \"sgRNA_20210907_BC19\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode19/basecalls.fastq\",\n",
    "              # \"sgRNA_20210907_BC20\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode20/basecalls.fastq\",\n",
    "              # \"sgRNA_20210907_BC21\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode21/basecalls.fastq\",\n",
    "              # \"sgRNA_20210913\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_09_13_alu1234/no_sample/20210913_1104_X3_FAQ85431_cc82f069/fastq_pass/basecalls.fastq\",\n",
    "              # \"sgRNA_20210916_sg14\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_09_16_alu_sg1_4_dG/no_sample/20210916_0734_X4_FAQ85431_492a7643/fastq_pass/basecalls.fastq\",\n",
    "              # \"sgRNA_20210916_sg24\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_09_16_alu_sg2_4_dA/no_sample/20210916_1011_X1_FAQ85121_54a631a8/fastq_pass/basecalls.fastq\"\n",
    "              }\n",
    "\n",
    "fastq_dict = {\n",
    "              # 'sgRNA_20210527': '/NAS/wg_looking/gRNA_ONT/data/no_sample/20210527_0710_X2_FAQ12460_0bbed906/KT_test/basecall_pass.fastq',\n",
    "              # 'sgRNA_20210617': '/NAS/wg_looking/gRNA_ONT/data/no_sample/20210617_0817_X2_FAQ14387_1b628e6b/basecall_pass.fastq',\n",
    "              # 'sgRNA_20210624': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/fastq_pass/barcode24/BC24.fastq',\n",
    "              # 'sgRNA_20210801_BC14': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode14/BC14.fastq',\n",
    "              # 'sgRNA_20210801_BC15': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode15/BC15.fastq',\n",
    "              # 'sgRNA_20210802': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_02_K562_aluL1_18UMI_2/no_sample/20210802_0412_X2_FAQ42413_d2eae930/basecall_pass.fastq',\n",
    "              # 'totalRNA_20210623_primer': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              # 'totalRNA_20210715_primer': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/20210715_1103_X2_FAQ42413_d69cb2d1/basecall_pass.fastq',\n",
    "              # 'totalRNA_20210803_primer': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_03_k562_totalRNA_18UMI-1-2/no_sample/20210803_0341_X2_FAQ61633_75e6e880/fastq_pass/basecall_pass.fastq',\n",
    "              # \"totalRNA_20210904_BC13_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode13/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC14_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode14/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC15_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode15/basecalls.fastq\",\n",
    "              # \"totalRNA_20210904_BC16_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210904_K562_RTtest/no_sample/20210904_0922_X3_FAQ85431_2f36a6fe/fastq_pass/barcode16/basecalls.fastq\",\n",
    "              # \"totalRNA_20210907_BC17_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode17/basecalls.fastq\",\n",
    "              # \"totalRNA_20210907_BC18_primer\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210907_Target_BarcodeTest_Number19-21/no_sample/20210907_0845_X4_FAQ85121_1d63aaaa/fastq_pass/barcode18/basecalls.fastq\",\n",
    "              \"totalRNA_20210901\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210901_K562_TotalRNA/no_sample/20210901_1037_X1_FAQ85934_fc086c2e/fastq_pass/basecalls.fastq\"\n",
    "}\n",
    "\n",
    "\n",
    "fastq_dict = {\n",
    "              \"totalRNA_MCF7\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_12_totalRNA_multicells/no_sample/20211012_1413_X4_FAQ93978_9eb8f7bc/fastq_pass/barcode13/basecall_pass.fastq\",\n",
    "              \"totalRNA_MCF10A\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_12_totalRNA_multicells/no_sample/20211012_1413_X4_FAQ93978_9eb8f7bc/fastq_pass/barcode14/basecall_pass.fastq\",\n",
    "              \"totalRNA_MRC5\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_12_totalRNA_multicells/no_sample/20211012_1413_X4_FAQ93978_9eb8f7bc/fastq_pass/barcode15/basecall_pass.fastq\",\n",
    "              \"totalRNA_H226\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_12_totalRNA_multicells/no_sample/20211012_1413_X4_FAQ93978_9eb8f7bc/fastq_pass/barcode16/basecall_pass.fastq\",\n",
    "              \"totalRNA_K562\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_12_totalRNA_multicells/no_sample/20211012_1413_X4_FAQ93978_9eb8f7bc/fastq_pass/barcode17/basecall_pass.fastq\"\n",
    "              }\n",
    "\n",
    "fastq_dict = {\n",
    "              \"totalRNA_MCF7_BC18\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_16_MCF-7_Total_BC18_22/no_sample/20211016_1244_X3_FAQ85508_8473f7bc/fastq_pass/barcode18/basecalls.fastq\",\n",
    "              \"totalRNA_MCF7_BC19\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_16_MCF-7_Total_BC18_22/no_sample/20211016_1244_X3_FAQ85508_8473f7bc/fastq_pass/barcode19/basecalls.fastq\",\n",
    "              \"totalRNA_MCF7_BC20\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_16_MCF-7_Total_BC18_22/no_sample/20211016_1244_X3_FAQ85508_8473f7bc/fastq_pass/barcode20/basecalls.fastq\",\n",
    "              \"totalRNA_MCF7_BC21\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_16_MCF-7_Total_BC18_22/no_sample/20211016_1244_X3_FAQ85508_8473f7bc/fastq_pass/barcode21/basecalls.fastq\",\n",
    "              \"totalRNA_MCF7_BC22\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_10_16_MCF-7_Total_BC18_22/no_sample/20211016_1244_X3_FAQ85508_8473f7bc/fastq_pass/barcode22/basecalls.fastq\"\n",
    "              }\n",
    "\n",
    "\n",
    "fastq_dict = {\n",
    "              \"totalRNA_K562_20211026_BC21\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211026_K562_RT_Test/no_sample/20211026_0731_X3_FAQ93978_bfd8f42b/fastq_pass/barcode21/basecalls.fastq\",\n",
    "              \"totalRNA_K562_20211026_BC22\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211026_K562_RT_Test/no_sample/20211026_0731_X3_FAQ93978_bfd8f42b/fastq_pass/barcode22/basecalls.fastq\",\n",
    "              }\n",
    "\n",
    "\n",
    "fastq_dict = {\n",
    "              # 'totalRNA_K562_1': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              # 'totalRNA_K562_2': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/20210715_1103_X2_FAQ42413_d69cb2d1/basecall_pass.fastq',\n",
    "              # 'totalRNA_K562_3': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_03_k562_totalRNA_18UMI-1-2/no_sample/20210803_0341_X2_FAQ61633_75e6e880/fastq_pass/basecall_pass.fastq',\n",
    "              \"sgRNA_K562_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_K562_Alu_L1/no_sample/20211029_1130_X5_FAR08063_519e8ce7/fastq_pass/basecall_pass.fastq\",  \n",
    "              # \"sgRNA_MDAMB231_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode16/basecall_pass.fastq\",\n",
    "              # \"sgRNA_MDAMB231_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode17/basecall_pass.fastq\",\n",
    "              # \"sgRNA_MDAMB231_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode18/basecall_pass.fastq\",           \n",
    "} \n",
    "\n",
    "fastq_dict = {\n",
    "              \"sgRNA_MCF10A_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode19/basecall_pass.fastq\",\n",
    "              \"sgRNA_MCF10A_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode20/basecall_pass.fastq\",\n",
    "              \"sgRNA_MCF10A_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode21/basecall_pass.fastq\",           \n",
    "} \n",
    "\n",
    "fastq_dict = {\n",
    "              \"sgRNA_HCT116_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210928_HCT116-1_Alu2_4_L1/no_sample/20210928_0140_X4_FAQ85508_e05a540f/fastq_pass/basecall_pass.fastq\",\n",
    "              \"sgRNA_HCT116_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210927_HCT116-2_Alu2_4_L1/no_sample/20210927_0544_X4_FAQ85508_98f41535/fastq_pass/basecall_pass.fastq\",\n",
    "              \"sgRNA_HCT116_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210929_HCT116-3_Alu2_4_L1/no_sample/20210929_0320_X4_FAQ85508_a6956ecd/fastq_pass/basecall_pass.fastq\",\n",
    "              # \"totalRNA_HCT116\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210928_HCT116_Total/no_sample/20210928_1114_X4_FAQ85508_2f8ae350/fastq_pass/basecall_pass.fastq\",\n",
    "              \"sgRNA_NCM460_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210929_NCM460-1_Alu2_4_L1/no_sample/20210929_0232_X3_FAQ93978_85b4dd11/fastq_pass/basecall_pass.fastq\",\n",
    "              \"sgRNA_NCM460_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210929_NCM460-2_Alu2_4_L1/no_sample/20210929_0836_X3_FAQ93978_0848952c/fastq_pass/basecall_pass.fastq\",\n",
    "              \"sgRNA_NCM460_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210928_NCM460-3_Alu2_4_L1/no_sample/20210928_0538_X3_FAQ93978_1e01562e/fastq_pass/basecall_pass.fastq\",\n",
    "              # \"totalRNA_NCM460\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210928_NCM460_Total/no_sample/20210928_1114_X3_FAQ93978_690e6e56/fastq_pass/basecall_pass.fastq\",\n",
    "              # \"totalRNA_NCM460_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20210901_NCM460_TotalRNA/no_sample/20210901_1037_X3_FAQ85121_536f8937/fastq_pass/basecall_pass.fastq\",\n",
    "              'totalRNA_K562_1': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              'totalRNA_K562_2': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/20210715_1103_X2_FAQ42413_d69cb2d1/basecall_pass.fastq',\n",
    "              'totalRNA_K562_3': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_03_k562_totalRNA_18UMI-1-2/no_sample/20210803_0341_X2_FAQ61633_75e6e880/fastq_pass/basecall_pass.fastq',\n",
    "              \"sgRNA_K562_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_K562_Alu_L1/no_sample/20211029_1130_X5_FAR08063_519e8ce7/fastq_pass/basecall_pass.fastq\",  \n",
    "              \"sgRNA_MDAMB231_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode16/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode17/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode18/basecall_pass.fastq\", \n",
    "              \"sgRNA_MCF10A_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode19/basecall_pass.fastq\",\n",
    "              \"sgRNA_MCF10A_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode20/basecall_pass.fastq\",\n",
    "              \"sgRNA_MCF10A_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211102_MCF10A_Alu_L1_BC19_21/no_sample/20211102_1102_X3_FAR07968_f1ea001b/fastq_pass/barcode21/basecall_pass.fastq\",           \n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "DNA_20210425_fastq = \"/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/20210425_1028_X2_FAQ12460_73ee278a/megalodon_results/basecalls.fastq\"\n",
    "DNA_20210623_fastq = \"/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/L1_methylation_K562_gDNA_lambdaDNA_test1_210623/no_sample/20210623_0811_X3_FAQ15107_7392dcdc/basecall_pass.fastq\"\n",
    "DNA_20210624_BC23_fastq = \"/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/KT_Test_barcode23/basecall_pass.fastq\"\n",
    "\n",
    "fastq_dict = dict(DNA_20210624_BC23=\"/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/basecall_pass/pass/BC23.fastq\"\n",
    "                 )\n",
    "\n",
    "\n",
    "fastq_dict = dict(nCATS_0812=\"/NAS/wg_looking/gRNA_ONT/data/no_sample/nCATS_8_12/CIP_8_12/basecall.fastq\",\n",
    "                  capTEs_0812=\"/NAS/wg_looking/gRNA_ONT/data/no_sample/capTEs_8_12/8_12/basecall.fastq\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering and trimming adapter of long reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate base\n",
    "def trimming_and_filtering(fastq, out_fq):\n",
    "    # cmd = \"NanoFilt -q 7 -l 300 --headcrop 10 --tailcrop 10 %s | \" \\\n",
    "    #   \"gzip > %s\" % (fastq, out_fq)\n",
    "    cmd = \"{porechop} -i {fastq} -t 40 | NanoFilt -q 7 -l 300 --headcrop 0 --tailcrop 0 | \" \\\n",
    "          \"gzip > {out_fq}\".format(porechop=porechop, fastq=fastq, out_fq=out_fq)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "pool = Pool(3)\n",
    "# for ID in [\"totalRNA_20210803\"]:\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    outdir = work_dir + \"/QC\"\n",
    "    fastq = fastq_dict[ID]\n",
    "    out_fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    pool.apply_async(trimming_and_filtering, args=(fastq, out_fq))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "# for DNA\n",
    "ID = \"DNA_20210624_BC23\"\n",
    "out_fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "trimming_and_filtering(DNA_20210624_BC23_fastq, out_fq)\n",
    "\n",
    "\n",
    "# # for total RNA\n",
    "# for ID in fastq_dict.keys():\n",
    "#     fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "#     tmp_fq = work_dir + \"/QC/%s.trimming.cut.Q7.L300.fq.gz\" % ID\n",
    "#     if ID in [\"totalRNA_20210803_primer\"]:\n",
    "#         num = 18\n",
    "#     else:\n",
    "#         num = 8\n",
    "#     cmd = \"gunzip -c %s | NanoFilt -q 7 -l 300 --headcrop %s --tailcrop %s | gzip > %s\" % (fq, num, num, tmp_fq)\n",
    "#     os.system(cmd)\n",
    "#     os.system(\"mv %s %s\" % (tmp_fq, fq))\n",
    "#     # os.system(\"rm -rf %s\" % fq[:-3])\n",
    "#\n",
    "\n",
    "# </editor-fold>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3'末端adapter的含量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate base\n",
    "\n",
    "porechop = \"/NAS/wg_looking/Tools/Porechop/porechop-runner.py\"\n",
    "\n",
    "fastq_dict = {\n",
    "              'totalRNA_20210623': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/smart3_total_RNA_seq_k562_210623/no_sample/20210623_0809_X2_FAQ13233_52704698/basecall_pass.fastq',\n",
    "              # 'sgRNA_20210624': '/NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/fastq_pass/barcode24/BC24.fastq',\n",
    "              'sgRNA_20210801_BC14': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode14/basecall_pass.fastq',\n",
    "              'sgRNA_20210801_BC15': '/NAS/wg_looking/gRNA_ONT/data/no_sample/2021_08_01_K562_aluL1_18UMI_1/no_sample/20210801_1058_X2_FAQ42413_0dcb0e44/fastq_pass/barcode15/basecall_pass.fastq',\n",
    "              \"sgRNA_MDAMB231_1\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode16/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_2\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode17/basecall_pass.fastq\",\n",
    "              \"sgRNA_MDAMB231_3\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_MDA_MB_231_BC16_18/no_sample/20211029_1130_X3_FAR07968_2b3e8113/fastq_pass/barcode18/basecall_pass.fastq\",\n",
    "              \"nCATS_0812\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/nCATS_8_12/CIP_8_12/basecall.fastq\",\n",
    "              \"capTEs_0812\": \"/NAS/wg_looking/gRNA_ONT/data/no_sample/capTEs_8_12/8_12/basecall.fastq\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "fastq_dict = dict(nCATS_0812=\"/NAS/wg_looking/gRNA_ONT/data/no_sample/nCATS_8_12/CIP_8_12/basecall.fastq\",\n",
    "                  capTEs_0812=\"/NAS/wg_looking/gRNA_ONT/data/no_sample/capTEs_8_12/8_12/basecall.fastq\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def trimming(fastq, out_fq):\n",
    "    cmd = \"{porechop} \\\n",
    "            -i {fastq} \\\n",
    "            --check_reads 10000 -t 30 \\\n",
    "            --verbosity 3 \\\n",
    "            --end_threshold 75 \\\n",
    "            -o {out_fq} > {out_fq}.score75.log\".format(\n",
    "        porechop=porechop, fastq=fastq, out_fq=out_fq)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{porechop} \\\n",
    "            -i {fastq} \\\n",
    "            --check_reads 10000 -t 30 \\\n",
    "            --verbosity 3 \\\n",
    "            --end_threshold 95 \\\n",
    "            -o {out_fq} > {out_fq}.score95.log\".format(\n",
    "        porechop=porechop, fastq=fastq, out_fq=out_fq)\n",
    "    os.system(cmd)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "pool = Pool(4)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    fastq = fastq_dict[ID]\n",
    "    outdir = work_dir + \"/porechop_log\"\n",
    "    SampleFq = outdir + \"/%s.random10000.fq\" % ID\n",
    "    if not os.path.exists(SampleFq):\n",
    "        cmd = \"/NAS/wg_looking/Tools/seqtk-1.3/seqtk sample -s 100 {totalFastq} 10000 > {out_fq}\".format(\n",
    "                out_fq=SampleFq, totalFastq=fastq)\n",
    "        os.system(cmd) \n",
    "    out_fq = outdir + \"/%s.random10000.trimming.fq\" % ID\n",
    "    pool.apply_async(trimming, args=(SampleFq, out_fq))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def porechop_trimmed_info(log, outfile):\n",
    "    # log = \"/NAS/wg_looking/gRNA_ONT/porechop_20210801_BC15.log\"\n",
    "    fin = open(log, \"r\")\n",
    "    lines = fin.readlines()\n",
    "    fin.close()\n",
    "    ###\n",
    "    readName_index = []\n",
    "    for line in lines:\n",
    "        if re.search(\"had adapters trimmed from their start\", line):\n",
    "            break\n",
    "        if re.search(r'runid', line):\n",
    "            index = lines.index(line)\n",
    "            readName_index.append(index)\n",
    "    ###\n",
    "    read_trimmed_info = {} # readname, start_Smart-seq3, start_SQK-NSK007, end_Smart-seq3, end_SQK-NSK007\n",
    "    for i in range(0, len(readName_index)):\n",
    "        index = readName_index[i]  \n",
    "        if i != (len(readName_index)-1):\n",
    "            index_latter = readName_index[i+1]\n",
    "            trimmed_info = lines[index:index_latter]\n",
    "        else:\n",
    "            trimmed_info=lines[index:(index+15)]\n",
    "        ####\n",
    "        # initialization\n",
    "        read_name = trimmed_info[0].split()[0]\n",
    "        read_trimmed_info[read_name] = {}\n",
    "        start_index = 0\n",
    "        end_index = 0\n",
    "        for n in [\"start_Smart-seq3\", \"start_SQK-NSK007\", \"end_Smart-seq3\", \"end_SQK-NSK007\"]:\n",
    "            read_trimmed_info[read_name][n] = 0\n",
    "        ##\n",
    "        # start alignments\n",
    "        for info in trimmed_info:\n",
    "            if re.search(r'start alignments', info):\n",
    "                start_index = trimmed_info.index(info)\n",
    "        # end alignments\n",
    "        for info in trimmed_info:\n",
    "            if re.search(r'end alignments', info):\n",
    "                end_index = trimmed_info.index(info)\n",
    "        # for start \n",
    "        if start_index != 0:\n",
    "            if end_index != 0:\n",
    "                sub = trimmed_info[start_index:end_index]  \n",
    "            else:\n",
    "                sub = trimmed_info[start_index:]\n",
    "            for info in sub:\n",
    "                if re.search(r'Smart-seq3', info):\n",
    "                    read_trimmed_info[read_name][\"start_Smart-seq3\"] = 1\n",
    "                if re.search(r'SQK-NSK007', info):\n",
    "                    read_trimmed_info[read_name][\"start_SQK-NSK007\"] = 1 \n",
    "        # for end\n",
    "        if end_index != 0 :\n",
    "            sub = trimmed_info[end_index:]\n",
    "            for info in sub:\n",
    "                if re.search(r'Smart-seq3', info):\n",
    "                    read_trimmed_info[read_name][\"end_Smart-seq3\"] = 1\n",
    "                if re.search(r'SQK-NSK007', info):\n",
    "                    read_trimmed_info[read_name][\"end_SQK-NSK007\"] = 1 \n",
    "    with open(outfile, \"w\") as fout:\n",
    "        fout.write(\"\\t\".join([\"read_name\", \"start_Smart-seq3\", \"start_SQK-NSK007\", \"end_Smart-seq3\", \"end_SQK-NSK007\"]) + \"\\n\")\n",
    "        for read_name in read_trimmed_info.keys():\n",
    "            fout.write(\"%s\\t%s\\t%s\\t%s\\t%s\\n\" % (\n",
    "                read_name, \n",
    "                str(read_trimmed_info[read_name][\"start_Smart-seq3\"]),\n",
    "                str(read_trimmed_info[read_name][\"start_SQK-NSK007\"]),\n",
    "                str(read_trimmed_info[read_name][\"end_Smart-seq3\"]),\n",
    "                str(read_trimmed_info[read_name][\"end_SQK-NSK007\"]),  \n",
    "            ))\n",
    "                           \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "pool = Pool(4)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    outdir = work_dir + \"/porechop_log\"\n",
    "    for threshold in [75, 95]:\n",
    "        log = outdir + \"/%s.random10000.trimming.fq.score%s.log\" % (ID, threshold)\n",
    "        out = log + \".txt\"\n",
    "        pool.apply_async(porechop_trimmed_info, args=(log, out, ))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "outdir = work_dir + \"/porechop_log\"\n",
    "pctFile = outdir + \"/end_smart_seq3_NSK007_adapter_pct.txt\"\n",
    "with open(pctFile, \"w\") as fout:\n",
    "    for ID in list(fastq_dict.keys()):\n",
    "        out75 = outdir + \"/%s.random10000.trimming.fq.score75.log.txt\" % ID\n",
    "        log95 = outdir + \"/%s.random10000.trimming.fq.score95.log.txt\" % ID\n",
    "        if os.path.exists(out75):\n",
    "            end_primer_reads = os.popen(\"awk '$4==1 {print $1}' %s\" % log95).read().split()\n",
    "            end_adapter = os.popen(\"awk '$5==1 {print $0}' %s\" % out75).read().split()\n",
    "            end_adapter_final = set(end_primer_reads).intersection(set(end_adapter)) \n",
    "            # total = int(os.popen(\"wc -l %s\" % out).read().split()[0]) - 1\n",
    "            pct = float(len(end_adapter_final))/len(end_primer_reads)\n",
    "            fout.write(\"%s\\t%s\\n\" % (ID, pct))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################\n",
    "#                        check PCR_primer                      #\n",
    "################################################################\n",
    "\n",
    "# <editor-fold desc=\"check PCR_primer\">\n",
    "fastq2fasta = \"/NAS/wg_looking/Tools/shasta_old/scripts/FastqToFasta.py\"\n",
    "db = \"/NAS/wg_looking/gRNA_ONT/database/PCR_primer\"\n",
    "# db = \"/NAS/wg_looking/gRNA_ONT/database/PCR_primer_smart_seq2\" # for sgRNA_20210527\n",
    "\n",
    "def blast_run(query, blast_out, db):\n",
    "    blastDir = \"/NAS/wg_looking/Tools/ncbi-blast-2.11.0+/bin\"\n",
    "    #\n",
    "    # make index for blast\n",
    "    # cmd = \"/NAS/wg_looking/Tools/ncbi-blast-2.11.0+/bin/makeblastdb -in /NAS/wg_looking/gRNA_ONT/database/PCR_primer_smart_seq2.fa -dbtype nucl -parse_seqids -out PCR_primer_smart_seq2\"\n",
    "    #\n",
    "    # start alignment\n",
    "    cmd = \"{blastDir}/blastn -task blastn-short -word_size 10 -strand plus \\\n",
    "    -db {db} \\\n",
    "    -query {query} \\\n",
    "    -out {blast_out} \\\n",
    "    -outfmt 6 -num_threads 50 -num_alignments 1000000000\".format(\n",
    "        blastDir=blastDir, db=db, query=query, blast_out=blast_out)\n",
    "    os.system(cmd)\n",
    "    # print(cmd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pool = Pool(10)\n",
    "for ID in fastq_dict.keys():\n",
    "# for ID in [\"sgRNA_20210527\"]:\n",
    "    fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    fa = work_dir + \"/QC/%s.trimming.Q7.L300.fa\" % ID\n",
    "    if not os.path.exists(fa):\n",
    "        cmd = \"gunzip -c {fq} | {fq2fa} - {fa}\".format(fq2fa=fastq2fasta, fq=fq, fa=fa)\n",
    "        os.system(cmd)\n",
    "        os.system(\"{samtools} faidx {fa}\".format(samtools=samtools, fa=fa))\n",
    "    if not os.path.exists(work_dir + \"/%s\" % ID):\n",
    "        os.mkdir(work_dir + \"/%s\" % ID)\n",
    "    blast_out = work_dir + \"/%s/%s.Q7.L300.blastn_fmt6.out\" % (ID, ID)\n",
    "    pool.apply_async(blast_run, args=(fa, blast_out, db, ))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "# </editor-fold>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# Mapping Long RNA-seq Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Mapping reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimap2_run(ref, fq, tobed):\n",
    "    sam = bam.split(\"sort.bam\")[0] + \"sam\"\n",
    "    cmd = \"{minimap2} -ax splice --MD {ref} {fq} > {sam}\".format(minimap2=minimap2, ref=ref, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} sort -@ 100 {sam} -o {bam}\".format(\n",
    "        samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} index {bam}\".format(\n",
    "        samtools=samtools, bam=bam)\n",
    "    os.system(cmd)\n",
    "\n",
    "    \n",
    "pool = Pool(5)\n",
    "for ID in fastq_dict.keys():\n",
    "    if not os.path.exists(work_dir + \"/\" + ID):\n",
    "        os.mkdir(work_dir + \"/\" + ID)\n",
    "    out_fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    pool.apply_async(minimap2_run, args=(hg38,out_fq, bam))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to transcriptom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_transcript_fa = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.fa\"\n",
    "\n",
    "def minimap2_transcriptom_run(transcript_fa, fq, bam):\n",
    "    sam = bam.split(\"sort.bam\")[0] + \"sam\"\n",
    "    cmd = \"{minimap2} -ax map-ont --MD {ref} {fq} > {sam}\".format(minimap2=minimap2, ref=transcript_fa, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} sort -@ 100 {sam} -o {bam}\".format(\n",
    "        samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} index {bam}\".format(\n",
    "        samtools=samtools, bam=bam)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "pool = Pool(3)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    if not os.path.exists(work_dir + \"/\" + ID):\n",
    "        os.mkdir(work_dir + \"/\" + ID)\n",
    "    out_fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    bam = work_dir + \"/%s/%s.transcriptom.sort.bam\" % (ID, ID)\n",
    "    pool.apply_async(minimap2_transcriptom_run, args=(ref_transcript_fa,out_fq, bam))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the Percentage of uniquely mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = work_dir + \"/stat/the_Percentage_of_uniquely_mapping_2.txt\"\n",
    "with open(outfile, \"a\") as fout:\n",
    "    for ID in list(fastq_dict.keys()):\n",
    "        bam = work_dir + \"/%s/%s.transcriptom.sort.bam\" % (ID, ID)\n",
    "        readFile = work_dir + \"/%s/%s.uniquely_mapping.txt\" % (ID, ID)\n",
    "        # cmd = \"%s view %s |awk '$2==0 || $2==16 || $2==256 || $2==272 \\\n",
    "        # {print $1}' |sort |uniq -c |awk '$1==1 {print $2}' > %s\" % (\n",
    "        #     samtools, bam, readFile)\n",
    "        cmd = \"%s view %s |awk '$5==60 {print $1}' |sort  > %s\" % (\n",
    "            samtools, bam, readFile)\n",
    "        os.system(cmd)\n",
    "        unique = int(os.popen(\"wc -l %s\" % readFile).read().split()[0])\n",
    "        fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "        total = int(os.popen(\"gunzip -c %s |wc -l \" % fq).read().split()[0])/4\n",
    "        pct = round(unique/total, 4)*100\n",
    "        fout.write(\"%s\\t%s\\n\" % (ID, pct))\n",
    "        fout.flush()\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Producing unique bam and bed  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_trans_bed = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.bed\"\n",
    "\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    bam = work_dir + \"/%s/%s.transcriptom.sort.bam\" % (ID, ID)\n",
    "    uniqueBam = work_dir + \"/%s/%s.transcriptom.sort.unique.bam\" % (ID, ID)\n",
    "    cmd = \"%s view -h  %s |awk '$1~/@/ || $5==60 {print $0}' | %s view -b  > %s\" % (\n",
    "        samtools, bam, samtools, uniqueBam )\n",
    "    os.system(cmd)\n",
    "    bed = work_dir + \"/%s/%s.transcriptom.sort.unique.bed\" % (ID, ID)\n",
    "    cmd = \"{bedtools} bamtobed -i {bam} > {bed}\".format(bedtools=bedtools, bam=uniqueBam, bed=bed)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the Percentage of uniquely mapping\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "def unique_mapping_rate(bam, fq):\n",
    "    cmd = \"%s view %s |awk '$5==60 {print $1}' |sort -u |wc -l\" % (samtools, bam)\n",
    "    unique = int(os.popen(cmd).read().split()[0])\n",
    "    total = int(os.popen(\"gunzip -c %s |wc -l \" % fq).read().split()[0])/4\n",
    "    pct = round(unique/float(total)*100, 2)\n",
    "    return(pct)\n",
    "    \n",
    "\n",
    "outfile = work_dir + \"/stat/the_Percentage_of_uniquely_mapping2.txt\"\n",
    "with open(outfile, \"w\") as fout:\n",
    "    fout.write(\"%s\\t%s\\t%s\\n\" % (\"ID\", \"mapping to genome\", \"mapping to transcriptom\"))\n",
    "    for ID in list(fastq_dict.keys()):\n",
    "        bam1 = work_dir + \"/%s/%s.transcriptom.sort.bam\" % (ID, ID)\n",
    "        bam2 = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "        fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "        fout.write(\"%s\\t%s\\t%s\\n\" % (ID, unique_mapping_rate(bam2, fq), unique_mapping_rate(bam1, fq)))\n",
    "        fout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cut sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import Bio\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import Align\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "# function\n",
    "def cut_sites(ID, mobile_element_name, mobile_element_fasta=\"NULL\", sequence=\"NULL\", winwidth = 80, min_score = 100):\n",
    "    fastq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    out1 = work_dir + \"/%s/read_start.fq\" % ID\n",
    "    out2 = work_dir + \"/%s/read_end.fq\" % ID\n",
    "    cmd = \"bash /NAS/wg_looking/gRNA_ONT/code/SeqExtract.sh %s %s %s %s\" % (fastq, out1, out2, 80)\n",
    "    os.system(cmd)\n",
    "    ### Store mobile element sequence and reverse complement\n",
    "    mobile_element = \"\"\n",
    "    if sequence != \"NULL\":\n",
    "        mobile_element = SeqRecord(Seq(sequence), id=mobile_element_name)\n",
    "    if mobile_element_fasta != \"NULL\":\n",
    "        mobile_element = SeqIO.read(mobile_element_fasta, \"fasta\")\n",
    "    mobile_element_seq = mobile_element.seq\n",
    "    rev_comp_mobile_element_seq = mobile_element_seq.reverse_complement()  # TCCGCCCGCCTCGGCCTCCCAAAGTGCTGGGATTACAGGCGTGAGCCACCGCGCCCGGCC\n",
    "    mobile_element_len = len(mobile_element_seq)\n",
    "    ### Create Aligner\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.mode = 'local'\n",
    "    aligner.match_score = 5\n",
    "    aligner.mismatch_score = -5\n",
    "    aligner.open_gap_score = -10\n",
    "    aligner.extend_gap_score = -5\n",
    "    ### Align reads to mobiole element sequence\n",
    "    read_fastq = work_dir + \"/%s/read_start.fq\" % ID\n",
    "    outfile = work_dir + \"/%s/read_start_cutsite_%s.txt\" % (ID, mobile_element_name)\n",
    "    with open(outfile, \"w\") as results_out:\n",
    "        i = 1  # Keep track of how many reads have been processed\n",
    "        #\n",
    "        for seq in SeqIO.parse(read_fastq, \"fastq\"):\n",
    "            # if i % 50 == 0:\n",
    "                # print(str(i) + \" reads processed\")\n",
    "            results_out.write(seq.id + \"\\n\")\n",
    "            #\n",
    "            # Use Bio.Align package to do pairwise alignments\n",
    "            #\n",
    "            forward_alignments = aligner.align(seq.seq[:winwidth], mobile_element_seq)\n",
    "            if not any(a.score >= min_score for a in forward_alignments):\n",
    "                results_out.write(\"No alignments with score >= 100 on the forward strand\" + \"\\n\")\n",
    "            else:\n",
    "                if ((forward_alignments[0].aligned)[0][0][0]) == 0:\n",
    "                    results_out.write(\"Forward alignment score: \" + \"\\t\" + str(forward_alignments[0].score) + \"\\n\")\n",
    "                    results_out.write(str(forward_alignments[0].aligned) + \"\\n\")\n",
    "                    results_out.write(\"Forward alignment starting mobile element base:\" + \"\\t\" + str(\n",
    "                        (forward_alignments[0].aligned)[1][0][0] + 1) + \"\\t\" + mobile_element_name + \"\\n\")\n",
    "                else:\n",
    "                    results_out.write(\"Forward strand alignment does not begin at start of read\" + \"\\n\")\n",
    "            #\n",
    "            rev_comp_alignments = aligner.align(seq.seq[:winwidth], rev_comp_mobile_element_seq)\n",
    "            if not any(a.score >= min_score for a in rev_comp_alignments):\n",
    "                results_out.write(\"No alignments with score >= 100 on the reverse strand\" + \"\\n\")\n",
    "                results_out.write(\"\\n\")\n",
    "            else:\n",
    "                if ((rev_comp_alignments[0].aligned)[0][0][0]) == 0:\n",
    "                    results_out.write(\n",
    "                        \"Reverse strand alignment score: \" + \"\\t\" + str(rev_comp_alignments[0].score) + \"\\n\")\n",
    "                    results_out.write(str(rev_comp_alignments[0].aligned) + \"\\n\")\n",
    "                    results_out.write(\n",
    "                        \"Reverse alignment starting mobile element base:\" + \"\\t\" + str(mobile_element_len - (\n",
    "                            (rev_comp_alignments[0].aligned)[1][0][0])) + \"\\t\" + mobile_element_name + \"\\n\")\n",
    "                    results_out.write(\n",
    "                        \"\\n\")  # subtracting the first alignment base from the size of the element to get the starting base with respect to the forward mobile element sequence\n",
    "                else:\n",
    "                    results_out.write(\"Reverse strand alignment does not begin at start of read\" + \"\\n\")\n",
    "                    results_out.write(\"\\n\")\n",
    "            i = i + 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### extract the first 80bp\n",
    "mobile_element_fasta = \"/NAS/wg_looking/gRNA_ONT/database/hg38reps.L1HS.fa\"\n",
    "cut_sites(\"DNA_20210623\", \"L1\", mobile_element_fasta, winwidth=80, min_score=100)\n",
    "cut_sites(\"sgRNA_20210802\", \"L1\", mobile_element_fasta, winwidth=80, min_score=100)\n",
    "for ID in [\"sgRNA_20210527\", \"sgRNA_20210617\"]:\n",
    "    cut_sites(ID, \"L1\", mobile_element_fasta, winwidth=80, min_score=100)\n",
    "\n",
    "###\n",
    "cut_sites(\"DNA_20210425\", \"Alu\", sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGA\", winwidth=30, min_score=80)\n",
    "cut_sites(\"sgRNA_20210913\", \"Alu\", sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGT\", winwidth=30, min_score=80)\n",
    "cut_sites(\"sgRNA_20210802\", \"Alu\", sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGT\", winwidth=30, min_score=80)\n",
    "cut_sites(\"sgRNA_20210916_sg14\", \"Alu\", sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGT\", winwidth=30, min_score=80)\n",
    "cut_sites(\"sgRNA_20210916_sg24\", \"Alu\", sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGT\", winwidth=30, min_score=80)\n",
    "\n",
    "\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    # if ID.split(\"_\")[0] == \"sgRNA\":\n",
    "    #     cut_sites(ID, \"Alu\",\n",
    "    #               sequence=\"GGCCGGGCGCGGTGGCTCACGCCTGTAATCCCAGCACTTTGGGAGGCCGAGGCGGGCGGATCACGAGGT\",\n",
    "    #               winwidth=30,\n",
    "    #               min_score=80)\n",
    "    #     print(ID + \": Alu finished\\n\")\n",
    "        mobile_element_fasta = \"/NAS/wg_looking/gRNA_ONT/database/hg38reps.L1HS.fa\"\n",
    "        cut_sites(ID, \"L1\", mobile_element_fasta, winwidth=80, min_score=100)\n",
    "        print(ID + \": L1 finished\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Saturation analysis ：sample reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/NAS/wg_looking/gRNA_ONT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_9960/3437996269.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mwork_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/NAS/wg_looking/gRNA_ONT\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mtotalFastq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_K562_Alu_L1/no_sample/20211029_1130_X5_FAR08063_519e8ce7/fastq_pass/basecall_pass.fastq\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/NAS/wg_looking/gRNA_ONT'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "def sample_reads(totalFastq, frac, times):\n",
    "    if frac==1:\n",
    "        frac = 0.9999\n",
    "    out_fq = work_dir + \"/Saturation_analysis/sgRNA_subsample_frac{frac}_times{times}.fastq\".format(\n",
    "        frac=frac,times=times)\n",
    "    length_txt = work_dir + \"/Saturation_analysis/sgRNA_subsample_frac{frac}_times{times}.length.txt\".format(\n",
    "        frac=frac, times=times)\n",
    "    if not os.path.exists(length_txt):\n",
    "        cmd = \"/NAS/wg_looking/Tools/seqtk-1.3/seqtk sample -s {seed} {totalFastq} {frac} > {out_fq}\".format(\n",
    "            seed=times, frac=frac, out_fq=out_fq, totalFastq=totalFastq)\n",
    "        os.system(cmd)\n",
    "        cmd = \"/NAS/wg_looking/Tools/seqkit fx2tab {fq} -l -H -i -n > {length_txt}\".format(\n",
    "            fq=out_fq, length_txt=length_txt)\n",
    "        os.system(cmd)\n",
    "        os.system(\"rm -rf %s\" % (out_fq))\n",
    "\n",
    "        \n",
    "def sample_reads_for_total(totalFastq, frac, times):\n",
    "    if frac==1:\n",
    "        frac = 0.9999\n",
    "    out_fq = work_dir + \"/Saturation_analysis/totalRNA_subsample_frac{frac}_times{times}.fastq\".format(\n",
    "        frac=frac,times=times)\n",
    "    length_txt = work_dir + \"/Saturation_analysis/totalRNA_subsample_frac{frac}_times{times}.length.txt\".format(\n",
    "        frac=frac, times=times)\n",
    "    if not os.path.exists(length_txt):\n",
    "        cmd = \"/NAS/wg_looking/Tools/seqtk-1.3/seqtk sample -s {seed} {totalFastq} {frac} > {out_fq}\".format(\n",
    "            seed=times, frac=frac, out_fq=out_fq, totalFastq=totalFastq)\n",
    "        os.system(cmd)\n",
    "        cmd = \"/NAS/wg_looking/Tools/seqkit fx2tab {fq} -l -H -i -n > {length_txt}\".format(\n",
    "            fq=out_fq, length_txt=length_txt)\n",
    "        os.system(cmd)\n",
    "        os.system(\"rm -rf %s\" % (out_fq))\n",
    "\n",
    "\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "os.chdir(work_dir)\n",
    "# sgRNA_K562_1\n",
    "totalFastq = \"/NAS/wg_looking/gRNA_ONT/data/no_sample/20211029_K562_Alu_L1/no_sample/20211029_1130_X5_FAR08063_519e8ce7/fastq_pass/basecall_pass.fastq\"\n",
    "pool = Pool(50)\n",
    "for frac in list(range(5,101,5)):\n",
    "    frac = frac/100\n",
    "    for times in list(range(1,51)):\n",
    "        pool.apply_async(sample_reads, args=(totalFastq, frac, times,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "### totalRNA\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "os.chdir(work_dir)\n",
    "totalFastq = \"Saturation_analysis/totalRNA_K562.fastq\"\n",
    "pool = Pool(50)\n",
    "for frac in list(range(5,101,5)):\n",
    "    frac = frac/100\n",
    "    for times in list(range(1,51)):\n",
    "        pool.apply_async(sample_reads_for_total, args=(totalFastq, frac, times,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quantifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### with reference transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "salmon = \"/NAS/wg_looking/Tools/salmon-1.5.2_linux_x86_64/bin/salmon\"\n",
    "gffread = \"/NAS/wg_looking/Tools/gffread-0.12.7.Linux_x86_64/gffread\"\n",
    "work_dir  = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "ref_transcript_fa = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.fa\"\n",
    "\n",
    "\n",
    "# prepare transcript fa\n",
    "cmd = \"{gffread} -w {outFa} -g {refFa} {gtf}\".format(\n",
    "    gffread=gffread, outFa=ref_transcript_fa, refFa=hg38, gtf=gtf)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "# make index\n",
    "transcript_index = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript_index\"\n",
    "cmd = \"{salmon} index -t {ref_transcript_fa} -i {transcript_index} -k 31\".format(\n",
    "    salmon=salmon, ref_transcript_fa=ref_transcript_fa, transcript_index=transcript_index)\n",
    "os.system(cmd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Quantifying in alignment-based mode\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    sam = work_dir + \"/%s/%s.transcriptom.sam\" % (ID, ID)\n",
    "    bam = work_dir + \"/%s/%s.transcriptom.bam\" % (ID, ID)\n",
    "    if not os.path.exists(bam):\n",
    "        cmd = \"{samtools} view -bS -h {sam} > {bam}\".format(samtools=samtools, sam=sam, bam=bam)\n",
    "        os.system(cmd)\n",
    "    salmonOut = work_dir + \"/%s/%s.salmon_quant\" % (ID, ID)\n",
    "    cmd = \"{salmon} quant -t {ref_transcript_fa} -g {gtf} -l U --ont -a {bam} -o {salmonOut}\".format(\n",
    "        salmon=salmon, ref_transcript_fa=ref_transcript_fa, gtf=gtf, bam=bam, salmonOut=salmonOut)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quantifying in mapping-based mode\n",
    "# salmonOut_mappingBased = work_dir + \"/%s/%s.mappingBased.salmon_quant\" % (ID, ID)\n",
    "# cmd = \"{salmon} quant -i {transcript_index} -l U -r {fq} --validateMappings -o {salmonOut}\".format(\n",
    "#     salmon=salmon, transcript_index=transcript_index, fq=fq, salmonOut=salmonOut_mappingBased)\n",
    "# os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with assembly transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "minimap2 = \"/NAS/wg_looking/Tools/minimap2-2.22_x64-linux/minimap2\"\n",
    "salmon = \"/NAS/wg_looking/Tools/salmon-1.5.2_linux_x86_64/bin/salmon\"\n",
    "gffread = \"/NAS/wg_looking/Tools/gffread-0.12.7.Linux_x86_64/gffread\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "work_dir  = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "os.chdir(work_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### sgRNA: HCT116 + NCM460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare transcript fa\n",
    "assemblyGTF = work_dir + \"/stringtie_res5/HCT116_NCM460/sgRNA_HCT116_NCM460.minCoverage_5.gap_20.gtf\"\n",
    "transcript_fa = work_dir + \"/stringtie_res5/HCT116_NCM460/sgRNA_HCT116_NCM460.minCoverage_5.gap_20.transcripts.fa\"\n",
    "\n",
    "if not os.path.exists(transcript_fa):\n",
    "    cmd = \"{gffread} -w {outFa} -g {refFa} {gtf}\".format(\n",
    "        gffread=gffread, outFa=transcript_fa, refFa=hg38, gtf=assemblyGTF)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "    \n",
    "# Quantifying in alignment-based mode\n",
    "for ID in [\"sgRNA_HCT116_1\", \"sgRNA_HCT116_2\", \"sgRNA_HCT116_3\", \"sgRNA_NCM460_1\", \"sgRNA_NCM460_2\", \"sgRNA_NCM460_3\"]:\n",
    "    fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    sam = work_dir + \"/%s/%s.HCT116_NCM460_transcriptom.sam\" % (ID, ID)\n",
    "    bam = work_dir + \"/%s/%s.HCT116_NCM460_transcriptom.bam\" % (ID, ID)\n",
    "    # if not os.path.exists(bam):\n",
    "    cmd = \"{minimap2} -ax map-ont --MD {ref} {fq} > {sam}\".format(\n",
    "        minimap2=minimap2, ref=transcript_fa, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} view -bS -h {sam} > {bam}\".format(samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    #\n",
    "    salmonOut = work_dir + \"/%s/%s.salmon_quant_with_HCT116_NCM460_gtf\" % (ID, ID)\n",
    "    cmd = \"{salmon} quant -t {ref_transcript_fa} -g {gtf} -l U --ont -a {bam} -o {salmonOut}\".format(\n",
    "        salmon=salmon, ref_transcript_fa=transcript_fa, gtf=assemblyGTF, bam=bam, salmonOut=salmonOut)\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### assembly gtf, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf_dict = dict(\n",
    "    sgK562 = \"stringtie_res5/K562/sgRNA_K562.minCoverage_5.gap_20.gtf\",\n",
    "    totalK562_1 = \"stringtie_res5/K562/totalRNA_K562_1.minCoverage_5.gap_20.gtf\",\n",
    "    totalK562 = \"stringtie_res5/K562/totalRNA_K562.minCoverage_5.gap_20.gtf\",\n",
    "    sgHCT116 = \"stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\",\n",
    "    totalHCT116 = \"stringtie_res5/HCT116/totalRNA_HCT116.minCoverage_5.gap_20.gtf\",\n",
    "    sgNCM460 = \"stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\",\n",
    "    totalNCM460 = \"stringtie_res5/NCM460/totalRNA_NCM460.minCoverage_5.gap_20.gtf\",\n",
    "    sgMDAMB231 = \"stringtie_res5/MDAMB231/sgRNA_MDAMB231.minCoverage_5.gap_20.gtf\",\n",
    "    sgMCF10A = \"stringtie_res5/MCF10A/sgRNA_MCF10A.minCoverage_5.gap_20.gtf\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make index\n",
    "for ID in gtf_dict.keys():\n",
    "    transcript_fa = work_dir + \"/stat/%s_assembly_transcripts.fa\" % ID\n",
    "    transcript_index = work_dir + \"/stat/%s_assembly_transcripts_index\" % ID\n",
    "    cmd = \"{salmon} index -t {ref_transcript_fa} -i {transcript_index} -k 31\".format(\n",
    "        salmon=salmon, ref_transcript_fa=transcript_fa, transcript_index=transcript_index)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    \n",
    "    \n",
    "# Quantifying in alignment-based mode\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    if ID == 'totalRNA_K562_1':\n",
    "        assemblyID = 'totalK562_1'\n",
    "    else:\n",
    "        assemblyID = ID.split(\"_\")[0][:-3] + ID.split(\"_\")[1]\n",
    "    transcript_fa = work_dir + \"/stat/%s_assembly_transcripts.fa\" % assemblyID\n",
    "    gtf = work_dir +  \"/\" + gtf_dict[assemblyID]\n",
    "    fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    sam = work_dir + \"/%s/%s.assembly_transcriptom.sam\" % (ID, ID)\n",
    "    bam = work_dir + \"/%s/%s.assembly_transcriptom.bam\" % (ID, ID)\n",
    "    # if not os.path.exists(bam):\n",
    "    cmd = \"{minimap2} -ax map-ont --MD {ref} {fq} > {sam}\".format(minimap2=minimap2, ref=transcript_fa, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} view -bS -h {sam} > {bam}\".format(samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    #\n",
    "    salmonOut = work_dir + \"/%s/%s.salmon_quant_with_assembly_gtf\" % (ID, ID)\n",
    "    cmd = \"{salmon} quant -t {ref_transcript_fa} -g {gtf} -l U --ont -a {bam} -o {salmonOut}\".format(\n",
    "        salmon=salmon, ref_transcript_fa=transcript_fa, gtf=gtf, bam=bam, salmonOut=salmonOut)\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate base\n",
    "\n",
    "gtf_dict = dict(\n",
    "    sgK562 = \"stringtie_res5/K562/sgRNA_K562.minCoverage_5.gap_20.gtf\",\n",
    "    totalK562_1 = \"stringtie_res5/K562/totalRNA_K562_1.minCoverage_5.gap_20.gtf\",\n",
    "    totalK562 = \"stringtie_res5/K562/totalRNA_K562.minCoverage_5.gap_20.gtf\",\n",
    "    sgHCT116 = \"stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\",\n",
    "    totalHCT116 = \"stringtie_res5/HCT116/totalRNA_HCT116.minCoverage_5.gap_20.gtf\",\n",
    "    sgNCM460 = \"stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\",\n",
    "    totalNCM460 = \"stringtie_res5/NCM460/totalRNA_NCM460.minCoverage_5.gap_20.gtf\",\n",
    "    sgMDAMB231 = \"stringtie_res5/MDAMB231/sgRNA_MDAMB231.minCoverage_5.gap_20.gtf\",\n",
    "    sgMCF10A = \"stringtie_res5/MCF10A/sgRNA_MCF10A.minCoverage_5.gap_20.gtf\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liqa_run(gtf, bam, out):\n",
    "    refgene = gtf + \".refgene\"\n",
    "    if not os.path.exists(refgene):\n",
    "        cmd = \"liqa -task refgene -ref {gtf} -format gtf -out {refgene}\".format(gtf=gtf, refgene=refgene)\n",
    "        os.system(cmd)\n",
    "    if not os.path.exists(out):\n",
    "        cmd = \"liqa -task quantify -refgene {gtf}.refgene -bam {bam} -out {out} -max_distance 20 -f_weight 1\".format(\n",
    "            gtf=gtf, bam=bam, out=out)\n",
    "        os.system(cmd)\n",
    "\n",
    "\n",
    "# Quantifying \n",
    "pool = Pool(5)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    if ID == 'totalRNA_K562_1':\n",
    "        assemblyID = 'totalK562_1'\n",
    "    else:\n",
    "        assemblyID = ID.split(\"_\")[0][:-3] + ID.split(\"_\")[1]\n",
    "    ##\n",
    "    gtf = work_dir +  \"/\" + gtf_dict[assemblyID]\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    out = work_dir + \"/%s/%s.liqa.quantify.txt\" % (ID, ID)\n",
    "    pool.apply_async(liqa_run, args=(gtf, bam, out,))\n",
    "    \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grep -e \"SINE/Alu\" -e \"LINE/L1\" /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.out | \\\n",
    "awk '$1>=225 && $2<=18 {OFS=\"\\t\"} $4=$5\"_\"$6\"_\"$7\":\"$10 {print $5,$6,$7,$4}'  > /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.TE.bed\n",
    "\n",
    "\n",
    "\n",
    "/NAS/wg_looking/Tools/pygtftk-1.5.3/bin/gtftk bed_to_gtf \\\n",
    "-i /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.TE.bed \\\n",
    "-o /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.TE.gtf \\\n",
    "-t transcript \\\n",
    "-s RM \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def liqa_run(gtf, bam, out):\n",
    "    refgene = gtf + \".refgene\"\n",
    "    if not os.path.exists(refgene):\n",
    "        cmd = \"liqa -task refgene -ref {gtf} -format gtf -out {refgene}\".format(gtf=gtf, refgene=refgene)\n",
    "        os.system(cmd)\n",
    "    if not os.path.exists(out):\n",
    "        cmd = \"liqa -task quantify -refgene {gtf}.refgene -bam {bam} -out {out} -max_distance 20 -f_weight 1\".format(\n",
    "            gtf=gtf, bam=bam, out=out)\n",
    "        os.system(cmd)\n",
    "\n",
    "\n",
    "# Quantifying \n",
    "pool = Pool(10)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    gtf = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.TE.gtf\"\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    out = work_dir + \"/%s/%s.liqa.TE.quantify.txt\" % (ID, ID)\n",
    "    pool.apply_async(liqa_run, args=(gtf, bam, out,))\n",
    "    \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## featureCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureCounts = \"/NAS/wg_looking/Tools/subread-2.0.3-Linux-x86_64/bin/featureCounts\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def featureCounts_transcript(gtf, bam, outFile, tmpDir):\n",
    "    cmd = \"{featureCounts} -a {gtf} \\\n",
    "    -o {out} {bam} \\\n",
    "    -F GTF -t transcript -g transcript_id \\\n",
    "    -f -O --minOverlap 20 -M --fraction \\\n",
    "    -s 0 -T 50 -L --maxMOp 10 \\\n",
    "    --tmpDir {tmpDir}\".format(\n",
    "    featureCounts=featureCounts, gtf=gtf, out=outFile, bam=bam, tmpDir=tmpDir)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "def featureCounts_gene(gtf, bam, outFile, tmpDir):\n",
    "    cmd = \"{featureCounts} -a {gtf} \\\n",
    "    -o {out} {bam} \\\n",
    "    -F GTF -t gene -g gene_id \\\n",
    "    -f -O --minOverlap 20 -M --fraction \\\n",
    "    -s 0 -T 50 -L --maxMOp 10 \\\n",
    "    --tmpDir {tmpDir}\".format(\n",
    "    featureCounts=featureCounts, gtf=gtf, out=outFile, bam=bam, tmpDir=tmpDir)\n",
    "    os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying with hg38 gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "\n",
    "# for transcripts\n",
    "pool = Pool(5)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    out = work_dir + \"/%s/%s.hg38.featureCounts\" % (ID, ID)\n",
    "    tmpDir = work_dir + \"/%s\" % (ID)\n",
    "    pool.apply_async(featureCounts_transcript, args=(gtf, bam, out, tmpDir,))\n",
    "    \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "# for genes\n",
    "pool = Pool(5)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    out = work_dir + \"/%s/%s.hg38.gene.featureCounts\" % (ID, ID)\n",
    "    tmpDir = work_dir + \"/%s\" % (ID)\n",
    "    pool.apply_async(featureCounts_gene, args=(gtf, bam, out, tmpDir,))\n",
    "    \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantifying with hg38 TE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TE_gtf = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.TE.gtf\"\n",
    "pool = Pool(5)\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    out = work_dir + \"/%s/%s.TE.featureCounts\" % (ID, ID)\n",
    "    tmpDir = work_dir + \"/%s\" % (ID)\n",
    "    pool.apply_async(featureCounts_run, args=(TE_gtf, bam, out, tmpDir,))\n",
    "    \n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callling variants: cuteSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuteSV = \"/NAS/wg_looking/Tools/cuteSV-cuteSV-v1.0.12/src/cuteSV/cuteSV\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "\n",
    "\n",
    "def cuteSV_pipeline(ID, bam, outVCF):\n",
    "    cmd = \"{cuteSV} --threads 50 \\\n",
    "            --report_readid \\\n",
    "            --min_read_len 300 \\\n",
    "            --max_split_parts -1 \\\n",
    "            --merge_ins_threshold 100 \\\n",
    "            --min_support 10 \\\n",
    "            --genotype \\\n",
    "            --min_size 300 \\\n",
    "            --diff_ratio_merging_INS 0.3 \\\n",
    "            --max_cluster_bias_INS 100 \\\n",
    "            --max_cluster_bias_DEL 100 \\\n",
    "            --diff_ratio_merging_DEL 0.3 \\\n",
    "            --sample {sampleID} \\\n",
    "            {sort_bam} {reference} {outVCF} {work_dir}\".format(\n",
    "        cuteSV=cuteSV, sampleID=ID, sort_bam=bam, reference=hg38, outVCF=outVCF, work_dir=\"./\")\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "        \n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    os.chdir(work_dir + \"/\" + ID)\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    outVCF = \"%s.cuteSV.vcf\" % (ID)\n",
    "    # outVCF = work_dir + \"/cuteSV/%s.cuteSV.vcf\" % (ID)\n",
    "    cuteSV_pipeline(ID, bam, outVCF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft-cliped reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate medaka\n",
    "import os\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "samclip = \"/NAS/wg_looking/Tools/samclip-0.4.0/samclip\"\n",
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "\n",
    "\n",
    "def assemblyRegion(ID, sam, clipped_region):\n",
    "    out_sam1 = sam[:-3] + \"cliped.tmp1.sam\"\n",
    "    out_sam2 = sam[:-3] + \"cliped.tmp2.sam\"\n",
    "    out_sam = sam[:-3] + \"cliped.sam\"\n",
    "    header_sam = sam[:-3] + \"header.sam\"\n",
    "    out_bam = sam[:-3] + \"cliped.sort.bam\"\n",
    "    out_bdg = sam[:-3] + \"cliped.bdg\"\n",
    "    filter_bdg = sam[:-3] + \"cliped.over20.bdg\"\n",
    "    merge_bed = sam[:-3] + \"cliped.merge.bed\"\n",
    "    cmd = \"{samclip} --ref {ref_fa} --max 6000 --invert < {input_sam} > {out_sam}\".format(\n",
    "        samclip=samclip, ref_fa=hg38, input_sam=sam, out_sam=out_sam1)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samclip} --ref {ref_fa} --max 20 --invert < {input_sam} > {out_sam}\".format(\n",
    "        samclip=samclip, ref_fa=hg38, input_sam=sam, out_sam=out_sam2)\n",
    "    os.system(cmd)\n",
    "    out_sam = work_dir + \"/%s/%s.Q7.L300.cliped.sam\" % (ID, ID)\n",
    "    cmd = \"sort {out_sam1} {out_sam2} {out_sam2} | uniq -u > {out_sam}\".format(\n",
    "        out_sam1=out_sam1, out_sam2=out_sam2, out_sam=out_sam)\n",
    "    os.system(cmd)\n",
    "    # header\n",
    "    cmd = \"head -n 640 {sam} > {header_sam}\".format(sam=sam, header_sam=header_sam)\n",
    "    os.system(cmd)\n",
    "    # final bam\n",
    "    cmd = \"cat {} {} > {}.tmp\".format(header_sam, out_sam, out_sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"mv {}.tmp {}\".format(out_sam, out_sam)\n",
    "    os.system(cmd)\n",
    "    # out_bam = sam[:-3] + \"cliped.sort.bam\"\n",
    "    cmd = \"{samtools} sort -@ 100 {sam} -o {bam}\".format(\n",
    "            samtools=samtools, sam=out_sam, bam=out_bam)\n",
    "    os.system(cmd)\n",
    "    # bdg\n",
    "    # out_bdg = sam[:-3] + \"cliped.bdg\"\n",
    "    cmd = \"{bedtools} genomecov -ibam {bam} -bg > {out_bdg}\".format(\n",
    "        bedtools=bedtools, bam=out_bam, out_bdg=out_bdg)\n",
    "    os.system(cmd)\n",
    "    # filter_bdg = sam[:-3] + \"cliped.over20.bdg\"\n",
    "    cmd = \"awk '$4>=20 {print $0}' %s > %s\" % (out_bdg, filter_bdg)\n",
    "    os.system(cmd)\n",
    "    # joint\n",
    "    # merge_bed = sam[:-3] + \"cliped.merge.bed\"\n",
    "    cmd = \"{bedtools} merge -i {bed} -d 20 > {merge_bed}\".format(\n",
    "        bedtools=bedtools, bed=filter_bdg, merge_bed=merge_bed)\n",
    "    os.system(cmd)\n",
    "    return(merge_bed)\n",
    "\n",
    " \n",
    "\n",
    "def PrepareReads(TotalFastq, bam, Assembly_dir, region, AimFq):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "    aim_sam = Assembly_dir + \"/aim.sam\"\n",
    "    ReadsID_file = '%s/ReadsID.txt' % (Assembly_dir)\n",
    "    cmd = \"{samtools} view {bam} \\\"{region}\\\" > {aim_sam}\".format(\n",
    "            samtools=samtools, bam=bam, region=region, aim_sam=aim_sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"awk '{print $1}' %s |sort -u > %s\" % (aim_sam, ReadsID_file)\n",
    "    os.system(cmd)       \n",
    "    cmd1 = '{seqkit} grep --pattern-file {ReadsID_file} {TotalFq} > {outFastq}'.format(\n",
    "            seqkit=seqkit, ReadsID_file=ReadsID_file, TotalFq=TotalFastq, outFastq=AimFq)\n",
    "    print(cmd1)\n",
    "    os.system(cmd1)\n",
    "    return(AimFq)\n",
    "\n",
    "\n",
    "def shasta_run(fastq, ShastaRunDir):\n",
    "    shasta = \"/NAS/wg_looking/Tools/shasta_old/shasta-Linux-0.7.0\"\n",
    "    if fastq[-3:] == \".gz\":\n",
    "        print(\"Shasta load reads from uncompressed FASTA and FASTQ files !!\")\n",
    "    else:\n",
    "        cmd = \"{shasta} --MarkerGraph.minCoverage 5 \\\n",
    "         --Reads.minReadLength 300 \\\n",
    "        --input {reads} --assemblyDirectory {outdir}\".format(\n",
    "            shasta=shasta, reads=fastq, outdir=ShastaRunDir)\n",
    "        os.system(cmd)\n",
    "    return(os.path.join(ShastaRunDir, \"Assembly.fasta\"))\n",
    "\n",
    "\n",
    "def racon_pipeline(assemblyFa_init, reads, raconDir, ID, n_itera=4):\n",
    "    minimap2 = \"/NAS/wg_looking/Tools/minimap2-2.22_x64-linux/minimap2\"\n",
    "    ##\n",
    "    if not os.path.exists(raconDir):\n",
    "        os.mkdir(raconDir)\n",
    "    for i in range(n_itera):\n",
    "        if i == 0:\n",
    "            assemblyFa = assemblyFa_init\n",
    "        else:\n",
    "            before_i = i-1\n",
    "            assemblyFa = \"{raconDir}/{itera}x/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "                          raconDir=raconDir, name=ID, itera=before_i)\n",
    "        iteraDir = \"{raconDir}/{itera}x\".format(\n",
    "            raconDir=raconDir, itera=i)\n",
    "        if not os.path.exists(iteraDir):\n",
    "            os.mkdir(iteraDir)\n",
    "        outSam = \"{iteraDir}/{name}_VS_Assembly.sam\".format(\n",
    "            iteraDir=iteraDir, name=ID)\n",
    "        cmd = \"{minimap2} -a -t 3 -x map-ont -k 15 \" \\\n",
    "              \"{assemblyFa} {reads} > {outSam}\".format(\n",
    "              minimap2=minimap2, assemblyFa=assemblyFa, reads=reads, outSam=outSam)\n",
    "        os.system(cmd)\n",
    "        outFa = \"{iteraDir}/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "            iteraDir=iteraDir, name=ID, itera=i)\n",
    "        cmd = \"racon -t 3 {reads} {outSam} {assemblyFa} \" \\\n",
    "              \"> {outFa}\".format(\n",
    "            reads=reads, outSam=outSam,\n",
    "            assemblyFa=assemblyFa, outFa=outFa)\n",
    "        os.system(cmd)\n",
    "    return(outFa)\n",
    "\n",
    "\n",
    "def medaka_run(assemblyFa, reads, medakaDir, ID):\n",
    "    cmd = \"medaka_consensus \\\n",
    "        -i {reads} \\\n",
    "        -d {assemblyFa} \\\n",
    "        -o {outDir} \\\n",
    "        -t 20 \\\n",
    "        -m r941_min_high_g360\".format(\n",
    "        reads=reads, assemblyFa=assemblyFa, outDir=medakaDir)\n",
    "    os.system(cmd)\n",
    "    #\n",
    "    cmd = \"medaka stitch \\\n",
    "        {medakaDir}/consensus_probs.hdf \\\n",
    "        {medakaDir}/consensus.fasta  {medakaDir}/{name}.assembly.fa\".format(\n",
    "        medakaDir=medakaDir, name=ID)\n",
    "    os.system(cmd)\n",
    "    return(\"{medakaDir}/{name}.assembly.fa\".format(medakaDir=medakaDir, name=ID))\n",
    "\n",
    "\n",
    "\n",
    "def AssemblyPIPE(TotalFastq, bam, workDir, region, regionName):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    # identify dir names\n",
    "    Assembly_dir = os.path.join(work_dir, ID, \"Clipped_assembly\", regionName)\n",
    "    if not os.path.exists(Assembly_dir):\n",
    "        os.makedirs(Assembly_dir)\n",
    "    ShastaRunDir = os.path.join(Assembly_dir, 'Shasta')\n",
    "    raconDir = os.path.join(Assembly_dir, 'racon')\n",
    "    medakaDir = os.path.join(Assembly_dir, 'medaka')\n",
    "    #\n",
    "    AimFq = Assembly_dir + \"/aim.fastq\"\n",
    "    ShastaAssembly = os.path.join(ShastaRunDir, \"Assembly.fasta\")\n",
    "    raconAssembly = os.path.join(raconDir, \"3x/polished_racon_{name}_3x.fasta\".format(name=regionName))\n",
    "    ###\n",
    "    if not os.path.exists(AimFq) and os.path.getsize(AimFq)==0:\n",
    "        AimFq = PrepareReads(TotalFastq, bam, Assembly_dir, region, AimFq) \n",
    "    if not os.path.exists(ShastaAssembly):\n",
    "        if os.path.exists(ShastaRunDir):\n",
    "            shutil.rmtree(ShastaRunDir)\n",
    "        # shasta assembly\n",
    "        ShastaAssembly = shasta_run(AimFq, ShastaRunDir) \n",
    "    if os.path.exists(ShastaAssembly):\n",
    "        if os.path.getsize(ShastaAssembly)==0:\n",
    "            shutil.rmtree(ShastaRunDir)\n",
    "            ShastaAssembly = shasta_run(AimFq, ShastaRunDir) \n",
    "    ###\n",
    "    if not os.path.exists(raconAssembly):  \n",
    "        if os.path.exists(raconDir):\n",
    "            shutil.rmtree(raconDir)\n",
    "        # racon assembly use shasta Assembly as init assembly\n",
    "        raconAssembly = racon_pipeline(ShastaAssembly, AimFq, raconDir, regionName)  \n",
    "    if os.path.exists(raconAssembly):\n",
    "        if os.path.getsize(raconAssembly)==0:\n",
    "            shutil.rmtree(raconDir)\n",
    "            raconAssembly = racon_pipeline(ShastaAssembly, AimFq, raconDir, regionName)   \n",
    "    # medaka consensus use racon Assembly as input\n",
    "    if os.path.getsize(raconAssembly):\n",
    "        if os.path.exists(medakaDir):\n",
    "            shutil.rmtree(medakaDir)\n",
    "        medakaAssembly = medaka_run(raconAssembly, AimFq, medakaDir, regionName)\n",
    "        # convert final assembly to tab file\n",
    "        if os.path.exists(medakaAssembly):\n",
    "            os.system('{seqkit} fx2tab -i {medakaAssembly} > {medakaAssembly}.tab'.format(\n",
    "                seqkit=seqkit, medakaAssembly=medakaAssembly))\n",
    "\n",
    "        \n",
    "def main(TotalFastq, ID, sam, AssemblyFasta):\n",
    "    bam = sam[:-3] + \"sort.bam\"\n",
    "    # Step1: get clipped region\n",
    "    clipped_region = sam[:-3] + \"cliped.merge.bed\"\n",
    "    if not os.path.exists(clipped_region):\n",
    "        assemblyRegion(ID, sam, clipped_region)\n",
    "    #\n",
    "    workDir = os.path.dirname(sam)\n",
    "    # \n",
    "    with open(clipped_region, \"r\") as fin:\n",
    "        pool = Pool(5)\n",
    "        for line in fin:\n",
    "            chrom = line.split()[0]\n",
    "            start = line.split()[1]\n",
    "            end = line.split()[2]\n",
    "            region = chrom + \":\" + start + \"-\" + end\n",
    "            regionName = chrom + \"_\" + start + \"_\" + end\n",
    "            medakaAssembly = os.path.join(workDir, \"Clipped_assembly\", regionName, 'medaka/{name}.assembly.fa.tab'.format(\n",
    "                                    name=regionName))\n",
    "            if not os.path.exists(medakaAssembly):\n",
    "                try:\n",
    "                    # AssemblyPIPE(TotalFastq, bam, workDir, region, regionName)\n",
    "                    pool.apply_async(AssemblyPIPE, args=(TotalFastq, bam, workDir, region, regionName,))\n",
    "                except:\n",
    "                    pass\n",
    "                    continue\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        del pool\n",
    "    ##  out fasta\n",
    "    with open(AssemblyFasta, \"w\") as OutPut:\n",
    "        for Name in os.listdir(workDir + \"/Clipped_assembly\"):\n",
    "            medakaAssembly = os.path.join(workDir, \"Clipped_assembly\", Name, 'medaka/{name}.assembly.fa.tab'.format(name=Name))\n",
    "            if os.path.exists(medakaAssembly):\n",
    "                with open(medakaAssembly, 'r') as fpin:\n",
    "                    for records in fpin.readlines():\n",
    "                        info = records.strip().split(\"\\t\")\n",
    "                        ASMID = '{Name}_{ID}'.format(Name=Name, ID=info[0])\n",
    "                        Seq = info[1]\n",
    "                        if len(Seq) > 500: # we only need assembly longer than 500 bp\n",
    "                            OutPut.write('>{ASMID}\\n{Seq}\\n'.format(ASMID=ASMID, Seq=Seq))\n",
    "                            OutPut.flush()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "for ID in fastq_dict.keys(): \n",
    "    ID = \"sgRNA_K562_1\"\n",
    "    TotalFastq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % (ID)\n",
    "    sam = work_dir + \"/%s/%s.Q7.L300.sam\" % (ID, ID)\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    AssemblyFasta = work_dir + '/%s/%s_softClipped_localAssembly.fa' % (ID, ID)\n",
    "    main(TotalFastq, ID, sam, AssemblyFasta)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping local Assembly fasta to reference\n",
    "import re\n",
    "cuteSV = '/NAS/wg_looking/Tools/cuteSV-cuteSV-v1.0.12/src/cuteSV/cuteSV'\n",
    "work_dir = '/NAS/wg_looking/gRNA_ONT'\n",
    "ID = \"sgRNA_K562_1\"\n",
    "\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    AssemblyFasta = work_dir + '/%s/%s_softClipped_localAssembly.fa' % (ID, ID)\n",
    "    sam = work_dir + '/%s/%s_softClipped_localAssembly.sam' % (ID, ID)\n",
    "    bam = work_dir + '/%s/%s_softClipped_localAssembly.sort.bam' % (ID, ID)\n",
    "    minimap2_run(hg38, AssemblyFasta, bam)\n",
    "    #\n",
    "    outINS = work_dir + '/%s/%s_softClipped_localAssembly_INS_info.bed' % (ID, ID)\n",
    "    with open(outINS, \"w\") as fout:\n",
    "        # fout.write(\"\\t\".join([\"INS_name\", \"INS_start\", \"INS_end\", \"chrom\", \"INS_pos\"]) + \"\\n\")\n",
    "        with open(sam, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                if line[0] == \"@\":\n",
    "                    continue\n",
    "                info = line.split()\n",
    "                INS_name = info[0]\n",
    "                Flag = int(info[1])\n",
    "                chrom = info[2]\n",
    "                tmp_INS_pos = int(info[3])\n",
    "                match_info = info[5]\n",
    "                contig_length = len(info[9])\n",
    "                if int(info[4])==60:\n",
    "                    match = re.sub( r\"([A-Z])\", r\"\\1 \", match_info).split()\n",
    "                    INS_index = [i for i in range(0, len(match)) if re.search(r'I', match[i])]\n",
    "                    H_index = [i for i in range(0, len(match)) if re.search(r'H', match[i])]\n",
    "                    for i in H_index:\n",
    "                        H_length = int(match[i][:-1])\n",
    "                        contig_length = contig_length + H_length\n",
    "                    for i in INS_index:\n",
    "                        INS_length = int(match[i][:-1])\n",
    "                        if INS_length > 50:\n",
    "                            print(i)\n",
    "                            read_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"D\" or m[-1] == \"N\":\n",
    "                                    continue\n",
    "                                read_location = read_location + int(m[:-1])\n",
    "                            ref_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                ref_location = ref_location + int(m[:-1])\n",
    "                            end_location = 0\n",
    "                            for m in match:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                end_location = end_location + int(m[:-1])\n",
    "                            ###   \n",
    "                            if Flag == 0 or Flag == 2048:\n",
    "                                INS_start = read_location\n",
    "                                INS_end = read_location + INS_length\n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            if Flag == 16 or Flag == 2064:\n",
    "                                INS_start = contig_length - read_location - INS_length\n",
    "                                INS_end = contig_length - read_location \n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            outinfo = [INS_name, str(INS_start), str(INS_end), chrom, str(INS_pos)]\n",
    "                            fout.write(\"\\t\".join(outinfo) + \"\\n\")\n",
    "    #\n",
    "    # os.chdir(work_dir + '/%s' % ID)\n",
    "    # outVCF = work_dir + '/%s/%s.softClipped.vcf' % (ID, ID)\n",
    "    # cmd = \"{cuteSV} --threads 50 \\\n",
    "    #         --report_readid \\\n",
    "    #         --min_read_len 300 \\\n",
    "    #         --max_split_parts -1 \\\n",
    "    #         --merge_ins_threshold 100 \\\n",
    "    #         --min_support 1 \\\n",
    "    #         --genotype \\\n",
    "    #         --min_size 50 \\\n",
    "    #         --diff_ratio_merging_INS 0.3 \\\n",
    "    #         --max_cluster_bias_INS 100 \\\n",
    "    #         --max_cluster_bias_DEL 100 \\\n",
    "    #         --diff_ratio_merging_DEL 0.3 \\\n",
    "    #         --sample {sampleID} \\\n",
    "    #         {sort_bam} {reference} {outVCF} {work_dir}\".format(\n",
    "    #     cuteSV=cuteSV, sampleID=ID, sort_bam=bam, reference=hg38, outVCF=outVCF, work_dir=\"./\")\n",
    "    # os.system(cmd)\n",
    "    ## run RepeatMasker on local Assembly fasta\n",
    "    outDir = work_dir + \"/\" + ID \n",
    "    cmd = \"{RepeatMasker} -pa 100 -a -species human {fa} -dir {outDir}\".format(\n",
    "        RepeatMasker=RepeatMasker, fa=AssemblyFasta, outDir=outDir)\n",
    "    os.system(cmd)\n",
    "    ##\n",
    "    cmd = \"awk '{OFS=\\\"\\t\\\"} $1>=225 && $2 <=18 && ($11==\\\"SINE/Alu\\\" || $11==\\\"LINE/L1\\\") {print $5,$6,$7,$10}' \\\n",
    "            %s.out > %s.RM.bed\" % (AssemblyFasta, AssemblyFasta[:-3])\n",
    "    os.system(cmd)\n",
    "    outBed = work_dir + '/%s/%s_softClipped_localAssembly_INS_TE.bed' % (ID, ID)\n",
    "    cmd = \"{bedtools} intersect -a {INS_bed} -b {prefix}.RM.bed -wa -wb > {out}\".format(\n",
    "        bedtools=bedtools, INS_bed=outINS, prefix=AssemblyFasta[:-3], out=outBed)\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Local Genome assembly based on reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate medaka\n",
    "import os,re\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "###############################################\n",
    "#              Function Part                \n",
    "###############################################\n",
    "def ParseVCF(SVvcf):\n",
    "    with open(SVvcf) as fin:\n",
    "        SV_anno = {}\n",
    "        for records in [x for x in fin.readlines() if not re.search('#', x)]:\n",
    "            Info = records.strip().split(\"\\t\")\n",
    "            ID = Info[2]\n",
    "            if  re.search(r'INS', ID):\n",
    "                SupportReads = [x.split('RNAMES=')[-1].split(\",\") for x in Info if re.search(r'RNAMES=', x)][0]\n",
    "                SV_anno[ID] = SupportReads\n",
    "    return(SV_anno)\n",
    "\n",
    "\n",
    "def shasta_run(fastq, ShastaRunDir):\n",
    "    shasta = \"/NAS/wg_looking/Tools/shasta_old/shasta-Linux-0.7.0\"\n",
    "    if fastq[-3:] == \".gz\":\n",
    "        print(\"Shasta load reads from uncompressed FASTA and FASTQ files !!\")\n",
    "    else:\n",
    "        cmd = \"{shasta} --MarkerGraph.minCoverage 5 \\\n",
    "         --Reads.minReadLength 300 \\\n",
    "        --input {reads} --assemblyDirectory {outdir}\".format(\n",
    "            shasta=shasta, reads=fastq, outdir=ShastaRunDir)\n",
    "        os.system(cmd)\n",
    "    return(os.path.join(ShastaRunDir, \"Assembly.fasta\"))\n",
    "\n",
    "\n",
    "def racon_pipeline(assemblyFa_init, reads, raconDir, ID, n_itera=4):\n",
    "    minimap2 = \"/NAS/wg_looking/Tools/minimap2-2.22_x64-linux/minimap2\"\n",
    "    ##\n",
    "    if not os.path.exists(raconDir):\n",
    "        os.mkdir(raconDir)\n",
    "    for i in range(n_itera):\n",
    "        if i == 0:\n",
    "            assemblyFa = assemblyFa_init\n",
    "        else:\n",
    "            before_i = i-1\n",
    "            assemblyFa = \"{raconDir}/{itera}x/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "                          raconDir=raconDir, name=ID, itera=before_i)\n",
    "        iteraDir = \"{raconDir}/{itera}x\".format(\n",
    "            raconDir=raconDir, itera=i)\n",
    "        if not os.path.exists(iteraDir):\n",
    "            os.mkdir(iteraDir)\n",
    "        outSam = \"{iteraDir}/{name}_VS_Assembly.sam\".format(\n",
    "            iteraDir=iteraDir, name=ID)\n",
    "        cmd = \"{minimap2} -a -t 3 -x map-ont -k 15 \" \\\n",
    "              \"{assemblyFa} {reads} > {outSam}\".format(\n",
    "              minimap2=minimap2, assemblyFa=assemblyFa, reads=reads, outSam=outSam)\n",
    "        os.system(cmd)\n",
    "        outFa = \"{iteraDir}/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "            iteraDir=iteraDir, name=ID, itera=i)\n",
    "        cmd = \"racon -t 3 {reads} {outSam} {assemblyFa} \" \\\n",
    "              \"> {outFa}\".format(\n",
    "            reads=reads, outSam=outSam,\n",
    "            assemblyFa=assemblyFa, outFa=outFa)\n",
    "        os.system(cmd)\n",
    "    return(outFa)\n",
    "\n",
    "\n",
    "def medaka_run(assemblyFa, reads, medakaDir, ID):\n",
    "    cmd = \"medaka_consensus \\\n",
    "        -i {reads} \\\n",
    "        -d {assemblyFa} \\\n",
    "        -o {outDir} \\\n",
    "        -t 2 \\\n",
    "        -m r941_min_high_g360\".format(\n",
    "        reads=reads, assemblyFa=assemblyFa, outDir=medakaDir)\n",
    "    os.system(cmd)\n",
    "    #\n",
    "    cmd = \"medaka stitch \\\n",
    "        {medakaDir}/consensus_probs.hdf \\\n",
    "        {medakaDir}/consensus.fasta  {medakaDir}/{name}.assembly.fa\".format(\n",
    "        medakaDir=medakaDir, name=ID)\n",
    "    os.system(cmd)\n",
    "    return(\"{medakaDir}/{name}.assembly.fa\".format(medakaDir=medakaDir, name=ID))\n",
    "\n",
    "\n",
    "def PrepareReads(TotalFastq, workDir, ReadIDList, SampleName):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    Assembly_dir = os.path.join(workDir, \"Assembly\", SampleName)\n",
    "    ReadsID_file = '%s/%s.ReadsID.txt' % (Assembly_dir, SampleName)\n",
    "    with open(ReadsID_file, 'w') as output:\n",
    "        print(\"Start to output Aim ReadsID on file %s\\n\\n\" % ReadsID_file)\n",
    "        for IDs in ReadIDList:\n",
    "            output.write(IDs +\"\\n\")\n",
    "            output.flush()\n",
    "    AimFastq = '%s/%s.fastq' % (Assembly_dir, SampleName)       \n",
    "    cmd1 = '{seqkit} grep --pattern-file {ReadsID_file} {TotalFq} > {outFastq}'.format(\n",
    "            seqkit=seqkit, ReadsID_file=ReadsID_file, TotalFq=TotalFastq, outFastq=AimFastq)\n",
    "    print(cmd1)\n",
    "    os.system(cmd1)\n",
    "    return(AimFastq)\n",
    "\n",
    "\n",
    "def AssemblyPIPE(TotalFastq, workDir, ReadIDList, SampleName):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    # identify dir names\n",
    "    Assembly_dir = os.path.join(workDir, \"Assembly\", SampleName)\n",
    "    if not os.path.exists(Assembly_dir):\n",
    "        os.makedirs(Assembly_dir)\n",
    "    ShastaRunDir = os.path.join(Assembly_dir, '%s_Shasta' % SampleName)\n",
    "    raconDir = os.path.join(Assembly_dir, '%s_racon' % SampleName)\n",
    "    medakaDir = os.path.join(Assembly_dir, '%s_medaka' % SampleName)\n",
    "    if os.path.exists(ShastaRunDir):                                        \n",
    "        os.rmdir(ShastaRunDir)\n",
    "        os.rmdir(os.path.join(Assembly_dir, '%s_racon' % SampleName))\n",
    "        os.rmdir(os.path.join(Assembly_dir, '%s_medaka' % SampleName))\n",
    "    else:\n",
    "        AimFq = PrepareReads(TotalFastq, workDir, ReadIDList, SampleName)   \n",
    "    # shasta assembly\n",
    "    ShastaAssembly = shasta_run(AimFq, ShastaRunDir)\n",
    "    # racon assembly use shasta Assembly as init assembly\n",
    "    raconAssembly = racon_pipeline(ShastaAssembly, AimFq, raconDir, SampleName)\n",
    "    # medaka consensus use racon Assembly as input\n",
    "    medakaAssembly = medaka_run(raconAssembly, AimFq, medakaDir, SampleName)\n",
    "    # convert final assembly to tab file\n",
    "    if os.path.exists(medakaAssembly):\n",
    "        os.system('{seqkit} fx2tab -i {medakaAssembly} > {medakaAssembly}.tab'.format(\n",
    "            seqkit=seqkit, medakaAssembly=medakaAssembly))\n",
    "\n",
    "\n",
    "def main(TotalFastq, SVvcf, SampleName, AssemblyFasta):\n",
    "    # Step1: parse vcf file get aim Reads for assembly\n",
    "    SV_dict = ParseVCF(SVvcf)\n",
    "    # Step2: make output dirs for every SV position make the WorkDir\n",
    "    if not os.path.exists(SampleName):\n",
    "        os.mkdir(SampleName)\n",
    "    workDir = os.path.join(os.getcwd(), SampleName)\n",
    "    for k,v in SV_dict.items():\n",
    "        try:\n",
    "            # pool.apply_async(AssemblyPIPE, args=(TotalFastq, workDir, v, k, ))\n",
    "            AssemblyPIPE(TotalFastq, workDir, v, k)\n",
    "        except:\n",
    "            pass\n",
    "            continue\n",
    "    OutPut = open(AssemblyFasta, 'w')\n",
    "    for Names in os.listdir(workDir + \"/Assembly\"):\n",
    "        medakaAssembly = os.path.join(workDir, \"Assembly\", Names, '{name}_medaka/{name}.assembly.fa.tab'.format(name=Names))\n",
    "        if os.path.exists(medakaAssembly):\n",
    "            with open(medakaAssembly, 'r') as fpin:\n",
    "                for records in fpin.readlines():\n",
    "                    info = records.strip().split(\"\\t\")\n",
    "                    ASMID = '{Name}_{ID}'.format(Name=Names, ID=info[0])\n",
    "                    Seq = info[1]\n",
    "                    if len(Seq) > 500: # we only need assembly longer than 500 bp\n",
    "                        OutPut.write('>{ASMID}\\n{Seq}\\n'.format(ASMID=ASMID, Seq=Seq))\n",
    "                        OutPut.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_37360/326060089.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mwork_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/NAS/wg_looking/gRNA_ONT\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwork_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mID\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"sgRNA_K562_1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mSVvcf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.cuteSV.vcf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mTotalFastq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwork_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/QC/%s.trimming.Q7.L300.fq.gz\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "os.chdir(work_dir)\n",
    "ID = \"sgRNA_K562_1\"\n",
    "\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    SVvcf = work_dir + \"/%s/%s.cuteSV.vcf\" % (ID, ID)\n",
    "    TotalFastq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    AssemblyFasta = work_dir + '/%s/%s_cuteSV_localAssembly.fa' % (ID, ID)\n",
    "    main(TotalFastq, SVvcf, ID, AssemblyFasta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping local Assembly fasta to reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "ID = \"sgRNA_K562_1\"\n",
    "\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    AssemblyFasta = work_dir + '/%s/%s_cuteSV_localAssembly.fa' % (ID, ID)\n",
    "    sam = work_dir + '/%s/%s_cuteSV_localAssembly.sam' % (ID, ID)\n",
    "    bam = work_dir + '/%s/%s_cuteSV_localAssembly.sort.bam' % (ID, ID)\n",
    "    minimap2_run(hg38, AssemblyFasta, bam)\n",
    "    #\n",
    "    outINS = work_dir + '/%s/%s_cuteSV_localAssembly_INS_info.bed' % (ID, ID)\n",
    "    with open(outINS, \"w\") as fout:\n",
    "        # fout.write(\"\\t\".join([\"INS_name\", \"INS_start\", \"INS_end\", \"chrom\", \"INS_pos\"]) + \"\\n\")\n",
    "        with open(sam, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                if line[0] == \"@\":\n",
    "                    continue\n",
    "                info = line.split()\n",
    "                INS_name = info[0]\n",
    "                Flag = int(info[1])\n",
    "                chrom = info[2]\n",
    "                tmp_INS_pos = int(info[3])\n",
    "                match_info = info[5]\n",
    "                contig_length = len(info[9])\n",
    "                if int(info[4])==60:\n",
    "                    match = re.sub( r\"([A-Z])\", r\"\\1 \", match_info).split()\n",
    "                    INS_index = [i for i in range(0, len(match)) if re.search(r'I', match[i])]\n",
    "                    H_index = [i for i in range(0, len(match)) if re.search(r'H', match[i])]\n",
    "                    for i in H_index:\n",
    "                        H_length = int(match[i][:-1])\n",
    "                        contig_length = contig_length + H_length\n",
    "                    for i in INS_index:\n",
    "                        INS_length = int(match[i][:-1])\n",
    "                        if INS_length > 50:\n",
    "                            print(i)\n",
    "                            read_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"D\" or m[-1] == \"N\":\n",
    "                                    continue\n",
    "                                read_location = read_location + int(m[:-1])\n",
    "                            ref_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                ref_location = ref_location + int(m[:-1])\n",
    "                            end_location = 0\n",
    "                            for m in match:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                end_location = end_location + int(m[:-1])\n",
    "                            ###   \n",
    "                            if Flag == 0 or Flag == 2048:\n",
    "                                INS_start = read_location\n",
    "                                INS_end = read_location + INS_length\n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            if Flag == 16 or Flag == 2064:\n",
    "                                INS_start = contig_length - read_location - INS_length\n",
    "                                INS_end = contig_length - read_location \n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            outinfo = [INS_name, str(INS_start), str(INS_end), chrom, str(INS_pos)]\n",
    "                            fout.write(\"\\t\".join(outinfo) + \"\\n\")\n",
    "    ## run RepeatMasker on local Assembly fasta\n",
    "    outDir = work_dir + \"/\" + ID \n",
    "    # RM_run(AssemblyFasta, outDir)\n",
    "    ##\n",
    "    cmd = \"awk '{OFS=\\\"\\t\\\"} $1>=225 && $2 <=18 && ($11==\\\"SINE/Alu\\\" || $11==\\\"LINE/L1\\\") {print $5,$6,$7,$10}' \\\n",
    "    %s.out > %s.RM.bed\" % (AssemblyFasta, AssemblyFasta[:-3])\n",
    "    os.system(cmd)\n",
    "    outBed = work_dir + '/%s/%s_cuteSV_localAssembly_INS_TE.bed' % (ID, ID)\n",
    "    cmd = \"{bedtools} intersect -a {INS_bed} -b {prefix}.RM.bed -wa -wb > {out}\".format(\n",
    "        bedtools=bedtools, INS_bed=outINS, prefix=AssemblyFasta[:-3], out=outBed)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                            \n",
    "                                \n",
    "                            \n",
    "                        \n",
    "                        \n",
    "                        \n",
    "            \n",
    "                \n",
    "                          \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# RepeatMasker run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "RepeatMasker = \"/NAS/wg_looking/Tools/RepeatMasker/RepeatMasker\"\n",
    "\n",
    "\n",
    "def RM_run(clean_fa, outDir):\n",
    "    # cmd = \"{RepeatMasker} -pa 100 -species human {fa} -dir {outDir}\".format(\n",
    "    #     RepeatMasker=RepeatMasker, fa=clean_fa, outDir=outDir)\n",
    "    cmd = \"{RepeatMasker} -pa 100 -a -species human {fa} -dir {outDir}\".format(\n",
    "        RepeatMasker=RepeatMasker, fa=clean_fa, outDir=outDir)\n",
    "    os.system(cmd)  # 可以使用-div 10，进一步保证重复序列的正确性.则结果会是div<10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## for small fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### for small fasta\n",
    "pool = Pool(3)\n",
    "for ID in list(fastq_dict.keys()): # .2\n",
    "    # if ID.split(\"_\")[0] == \"sgRNA\":\n",
    "        # clean_fa = work_dir + \"/%s/%s.Q7.L300_clean.fa\" % (ID, ID)\n",
    "        fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "        outfa = work_dir + \"/QC/%s.trimming.Q7.L300.fa\" % ID\n",
    "        os.system(\"/NAS/wg_looking/Tools/seqkit fq2fa %s -j 50 -o %s\" % (fq, outfa))\n",
    "        outDir = work_dir + \"/%s/RepeatMasker_res_Q7_L300\" % ID\n",
    "        if not os.path.exists(outDir):\n",
    "            os.makedirs(outDir)\n",
    "        os.chdir(outDir)\n",
    "        pool.apply_async(RM_run, args=(outfa, outDir,))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## for large fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pool = Pool(5)\n",
    "for ID in fastq_dict.keys(): # .2\n",
    "    fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "    outfa = work_dir + \"/QC/%s.trimming.Q7.L300.fa\" % ID\n",
    "    if not os.path.exists(outfa):\n",
    "        os.system(\"/NAS/wg_looking/Tools/seqkit fq2fa %s -j 50 -o %s\" % (fq, outfa))\n",
    "    outDir = work_dir + \"/%s/RepeatMasker_res_Q7_L300\" % ID\n",
    "    splitfa_dir = work_dir + \"/%s/split_fa\" % ID\n",
    "    if not os.path.exists(splitfa_dir):\n",
    "        os.makedirs(splitfa_dir)\n",
    "    cmd = \"split -l 500000 {fa} {dir}/\".format(fa=outfa, dir=splitfa_dir)\n",
    "    os.system(cmd)\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "    for fa in os.listdir(work_dir + \"/%s/split_fa\" % ID):\n",
    "        os.chdir(outDir)\n",
    "        if os.path.exists(fa + \".out\"):\n",
    "            continue\n",
    "        input_fa = work_dir + \"/%s/split_fa/%s\" % (ID, fa)\n",
    "        pool.apply_async(RM_run, args=(input_fa, outDir,))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n",
    "## merge out\n",
    "for ID in fastq_dict.keys():\n",
    "    outDir = work_dir + \"/%s/RepeatMasker_res_Q7_L300\" % ID\n",
    "    os.chdir(outDir)\n",
    "    os.system(\"less -S aa.out |head -n 3 > %s.Q7.L300_clean.fa.rm.tmp\" % ID)\n",
    "    for file in os.listdir(outDir):\n",
    "        if file.count(\"out\") == 1:\n",
    "            os.system(\"tail -n +4 %s >> %s.Q7.L300_clean.fa.rm.tmp\" % (file, ID))\n",
    "    os.system(\"mv %s.Q7.L300_clean.fa.rm.tmp %s.trimming.Q7.L300.fa.out\" % (ID, ID))\n",
    "    os.system(\"chmod -R 777 %s\" % outDir)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## for DNA\n",
    "ID = \"DNA_20210623\"\n",
    "outDir = work_dir + \"/%s/RepeatMasker_res_Q7_L300\" % ID\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "\n",
    "fq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % ID\n",
    "fa = work_dir + \"/QC/%s.trimming.Q7.L300.fa\" % ID\n",
    "os.system(\"/NAS/wg_looking/Tools/seqkit fq2fa %s -j 50 -o %s\" % (fq, fa))\n",
    "os.chdir(outDir)\n",
    "RM_run(fa, outDir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# detects non-reference MEI events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xTea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate base\n",
    "# source .bashrc\n",
    "# import sortedcontainers\n",
    "import os\n",
    "\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "xTea = \"/NAS/wg_looking/Tools/xTea/bin/xtea_long\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "rep_lib = \"/NAS/wg_looking/Tools/xTea/rep_lib_annotation/\"\n",
    "hg38_rmsk = \"/NAS/wg_looking/Tools/xTea/rep_lib_annotation/Alu/hg38/hg38_Alu.out\"\n",
    "hg38_gff = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gff3\"\n",
    "\n",
    "\n",
    "os.chdir(work_dir)\n",
    "work_tmp = \"xTea_res\"\n",
    "bam_list = \"long_read_bam_list.txt\" # .bai is needed\n",
    "sampleTXT = \"xTea_sample_id.txt\"\n",
    "cmd = \"/NAS/wg_looking/Tools/xTea/bin/xtea_long -i {sampleFile} -b {bam_list} \" \\\n",
    "      \"-p {work_tmp} -o submit_jobs.sh --rmsk {rmsk} \" \\\n",
    "      \"-r {genome_fa} --cns /NAS/wg_looking/Tools/xTea/rep_lib_annotation/consensus \" \\\n",
    "      \"--rep {rep_lib} --xtea /NAS/wg_looking/Tools/xTea/xtea_long \" \\\n",
    "      \"-g {gff} -f 31 -y 3 --clean\".format(\n",
    "    sampleFile=sampleTXT, bam_list=bam_list, work_tmp=work_tmp, rmsk=hg38_rmsk, \n",
    "    genome_fa=hg38, rep_lib=rep_lib, gff=hg38_gff)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "os.chdir(work_dir)\n",
    "for ID in [\"sgRNA_HCT116_1\", \"sgRNA_HCT116_2\", \"sgRNA_HCT116_3\", \"totalRNA_HCT116\"]:\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    if not os.path.exists(bam + \".bai\"):\n",
    "        os.system(\"samtools index %s\" % bam)\n",
    "    subdir = \"xTea_res/%s\" % ID\n",
    "    os.system(\"sh {subdir}/run_xTEA_pipeline.sh > {subdir}/xTEA_test.log \\\n",
    "               2> {subdir}/xTEA_test.log2\".format(subdir=subdir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## PALMER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "fai = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa.fai\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "PALMER=\"/NAS/wg_looking/Tools/PALMER-1.7.2/PALMER\"\n",
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "def PALMER_run(ID, bam, TE_type, outDir):\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "    out_prefix = ID + \".PALMER.\" + TE_type\n",
    "    cmd = \"{PALMER} --input {bam} \" \\\n",
    "          \"--workdir {tmp_dir} \" \\\n",
    "          \"--ref_ver GRCh38 \" \\\n",
    "          \"--ref_fa {fasta} \" \\\n",
    "          \"--type {TE_type} \" \\\n",
    "          \"--chr ALL \" \\\n",
    "          \"--TSD_finding TRUE \" \\\n",
    "          \"--output {out_prefix} > {tmp_dir}{out_prefix}.log 2>&1\".format(\n",
    "        PALMER=PALMER, bam=bam, tmp_dir=outDir, fasta=hg38, TE_type=TE_type, out_prefix=out_prefix)\n",
    "    os.system(cmd)\n",
    "    \n",
    "\n",
    "\n",
    "pool = Pool(5)\n",
    "for ID in fastq_dict.keys():\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID) # index bam\n",
    "    for TE in [\"ALU\", \"LINE\"]:\n",
    "        outDir = work_dir + \"/%s/PALMER_res/%s/\" % (ID, TE)   # Create in advance\n",
    "        pool.apply_async(PALMER_run, args=(ID, bam, TE, outDir,))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_dict = {\"sgRNA_HCT116\":['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3'],\n",
    "           \"totalRNA_HCT116\":[\"totalRNA_HCT116\"],\n",
    "           \"sgRNA_NCM460\":['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3'],\n",
    "           \"totalRNA_NCM460\":['totalRNA_NCM460', 'totalRNA_NCM460_2'],\n",
    "           \"sgRNA_K562_1\":[\"sgRNA_K562_1\"],\n",
    "           \"totalRNA_K562\":['totalRNA_K562_1', 'totalRNA_K562_2', 'totalRNA_K562_3'],\n",
    "           \"sgRNA_MDAMB231\":['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3'],\n",
    "           \"sgRNA_MCF10A\":['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3'] \n",
    "          }\n",
    "\n",
    "\n",
    "for SeqCell in ID_dict.keys():\n",
    "    bams = []\n",
    "    for ID in ID_dict[SeqCell]:\n",
    "        bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "        bams.append(bam)\n",
    "    merge_bam = work_dir + \"/merge_PALMER/%s.merge.bam\" % SeqCell\n",
    "    cmd = \"{samtools} merge {merge_bam} {input_bam}\".format(\n",
    "        samtools=samtools, merge_bam=merge_bam, input_bam=\" \".join(bams))\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} index {bam}\".format(samtools=samtools, bam=merge_bam)\n",
    "    os.system(cmd)\n",
    "        \n",
    "    \n",
    "    \n",
    " \n",
    "## run PALMER\n",
    "pool = Pool(4)\n",
    "for SeqCell in ID_dict.keys():\n",
    "    bam = work_dir + \"/merge_PALMER/%s.merge.bam\" % SeqCell # index bam\n",
    "    for TE in [\"ALU\", \"LINE\"]:\n",
    "        outDir = work_dir + \"/merge_PALMER/%s/%s/\" % (SeqCell, TE)   # Create in advance\n",
    "        pool.apply_async(PALMER_run, args=(SeqCell, bam, TE, outDir,))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in {2..22} \n",
    "do\n",
    "samtools view -b totalRNA_K562.merge.bam \"chr\"$i > \"totalRNA_K562.merge.chr\"$i\".bam\"\n",
    "samtools index \"totalRNA_K562.merge.chr\"$i\".bam\"\n",
    "done\n",
    "\n",
    "\n",
    "samtools view -b totalRNA_K562.merge.bam chrX > \"totalRNA_K562.merge.chrX.bam\"\n",
    "samtools index \"totalRNA_K562.merge.chrX.bam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/NAS/wg_looking/gRNA_ONT/merge_PALMER/sgRNA_K562_1/ALU\n",
    "chr3_154000001_155000000\n",
    "\n",
    "cat chr*_*_*/blastn_refine.txt >>  blastn_refine.all.txt \n",
    "awk '{print $1}' blastn_refine.all.txt |sort -u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awk '$12>=50 {print $0}' sgRNA_K562_1.PALMER.ALU_calls.txt |head -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id      chr start1      start2  end1    end2    start1_inVariant        start2_inVariant        end1_inVariant  end2_inVariant      Confident_supporting_reads      Potential_supporting_reads      Ptential_segmental_supporting_reads     orientation     polyA-tail_size     5'_TSD_size     3'_TSD_size     Predicted_transD_size   Has_5'_inverted_sequence?       5'_inverted_seq_end     5'_seq_start\n",
    "cluster5_chr2_128191426_128191871_128191689_128191871   chr2    128191426       128191871       128191689       128191871       3  3\n",
    "        280     312     0       54      53      -       -       0       0       0       0       0       0\n",
    "cluster23_chr2_128193270_128193325_128193270_128193325  chr2    128193270       128193325       128193270       128193325       108115      297     313     0       54      54      +       -       0       0       0       0       0       0\n",
    "cluster1_chr3_10150965_10151002_10150965_10151002       chr3    10150965        10151002        10150965        10151002        28 50       296     321     0       80      70      +       -       0       0       0       0       0       0\n",
    "cluster2_chr12_54240407_54240447_54240407_54240447      chr12   54240407        54240447        54240407        54240447        1  1\n",
    "        331     331     0       83      83      +       -       0       0       0       0       0       0\n",
    "cluster5_chr12_54240465_54240496_54240465_54240496      chr12   54240465        54240496        54240465        54240496        1  6\n",
    "        319     320     0       67      67      +       -       0       0       0       0       0       0\n",
    "cluster87_chr12_132770712_132770748_132770712_132770748 chr12   132770712       132770748       132770712       132770748       119119      281     320     0       83      83      +       -       0       0       0       0       0       0\n",
    "cluster0_chr14_74200120_74200155_74200123_74200155      chr14   74200120        74200155        74200123        74200155        28 42       302     310     0       80      72      +       -       0       0       0       0       0       0\n",
    "cluster157_chr17_39209313_39389674_39248104_39389689    chr17   39209313        39389674        39248104        39389689        1  1\n",
    "        280     323     0       65      64      -       -       0       0       0       0       0       0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_id      chr start1      start2  end1    end2    start1_inVariant        start2_inVariant        end1_inVariant  end2_inVariant      Confident_supporting_reads      Potential_supporting_reads      Ptential_segmental_supporting_reads     orientation     polyA-tail_size     5'_TSD_size     3'_TSD_size     Predicted_transD_size   Has_5'_inverted_sequence?       5'_inverted_seq_end     5'_seq_start\n",
    "cluster36_chr1_1388182_1389882_1388277_1389882  chr1    1388182 1389882 1388277 1389882 1       1       298     314     0       7  6   -       -       0       0       0       0       0       0\n",
    "chr1_1000001_2000000\n",
    "185.309.1913.2035.1388182.1389882.chr1.-.0.0.0.0.0.0.0.junc.ref.fasta: chr1:1387832-1390232 # 以插入位点，向两边扩展350bp\n",
    "185.309:Alu上，比对上插入序列的位置\n",
    "1913.2035：插入序列上，比对到Alu的位置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"TTGTTTTTGTTTTTGTTTGTGTTTTTTGGAGACGGAGCTGCTTTGTTGCCCAAGCTGGAGTGCAATGGCGCAATCTCAGCTCACTGCAACCTCCACCTCCCAGGGTCAAGCGATTCTCCTGCC\")\n",
    "len(\"GTCAAGCGATTCTCCTGCCTCAGGAGGCGGAGCTTGCAGTGAGGAAATTGTGCCACTGCACTCCAGCCTGGGCTGGAGCAAGACTCTGTCTCAAACAAAAAGAATTAGCTAAGCNNNNNNNNNNNNNNNNNNNNNNNNN\")\n",
    "len('ACATGCTCCCCTCCTTTCTTTCAGCCTCAAAAACCATTGCCCCGGGATACAAACCTATGAAGACTGAAAGACTGCTGTTTCTCTCCAACTGAGCTCTAAGAATTTCCTTCTTCCATGAGGACTGTGATCATTACAAAATCTGGAACATCGACTACATTTGATTTACGGATACACAGAAAATTTTGTGTACAGGTGGGGAAT')\n",
    "len('TCGACGACTGCTGGCCTGAGTGCAGTGGCGCGATCTCAGCTCACTGCAATCTCCGTCTCCGGGTTCACGCCATTCTCCGCCTCAGTGAGCCAAGATCGGTGCCACTTCACTCCAGCCTGAGCAACATGAGACGGGGTCTAAAAAAAAAGGAACAAAAACACAAATTGGCTACTGACAGGACACAAAGTGCCTACTACCTGTGACTCAACTGTAC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2035-1913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "309-185"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat chr*_*_*/blastn_refine.txt >>  blastn_refine.all.txt \n",
    "\n",
    "# awk -F \"_\" '{print $1}' read_result_ins_seq.txt |sort -u > ReadsID.txt\n",
    "awk '{print $2}' blastn_refine.all.txt  | awk -F '_' '{print $1}' |sort -u > ReadsID.txt\n",
    "\n",
    "\n",
    "cd chr2_128000001_129000000\n",
    "awk '{print $2}' blastn_refine.txt  | awk -F '_' '{print $1}' |sort -u > ReadsID.txt\n",
    "\n",
    "\n",
    "/NAS/wg_looking/Tools/seqkit grep --pattern-file ReadsID.txt \\\n",
    "/NAS/wg_looking/gRNA_ONT/QC/sgRNA_K562_1.trimming.Q7.L300.fq.gz > aim.fq\n",
    "\n",
    "# /NAS/wg_looking/Tools/canu-2.2/bin/canu -correct \\\n",
    "#    -p canu_test1 \\\n",
    "#    -d ./ \\\n",
    "#    genomeSize=15M \\\n",
    "#    -nanopore -raw aim.fq\n",
    "\n",
    "\n",
    "/NAS/wg_looking/Tools/canu-2.2/bin/canu -correct \\\n",
    "   -p canu_test1 \\\n",
    "   -d ./ \\\n",
    "   genomeSize=1k \\\n",
    "   -nanopore -raw aim.fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "\n",
    "def minimap2_run(ref, fq, tobed):\n",
    "    sam = bam.split(\"sort.bam\")[0] + \"sam\"\n",
    "    cmd = \"{minimap2} -ax splice --MD {ref} {fq} > {sam}\".format(minimap2=minimap2, ref=ref, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} sort -@ 100 {sam} -o {bam}\".format(\n",
    "        samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} index {bam}\".format(\n",
    "        samtools=samtools, bam=bam)\n",
    "    os.system(cmd)\n",
    "    \n",
    "    \n",
    "minimap2 -ax splice --MD /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa \\\n",
    "canu_test1.correctedReads.fasta.gz > canu_test1.correctedReads.sam\n",
    "    \n",
    "# cat *junc.ref.fasta > junc.ref.fasta\n",
    "# minimap2 -ax splice --MD junc.ref.fasta \\\n",
    "# canu_test1.correctedReads.fasta.gz > canu_test1.correctedReads.sam\n",
    "\n",
    "samtools sort -@ 100 canu_test1.correctedReads.sam -o canu_test1.correctedReads.bam\n",
    "samtools index canu_test1.correctedReads.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PALMER=\"/NAS/wg_looking/Tools/PALMER-1.7.2/PALMER\"\n",
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "def PALMER_run(ID, bam, TE_type, outDir):\n",
    "    if not os.path.exists(outDir):\n",
    "        os.makedirs(outDir)\n",
    "    out_prefix = ID + \".PALMER.\" + TE_type\n",
    "    cmd = \"{PALMER} --input {bam} \" \\\n",
    "          \"--workdir {tmp_dir} \" \\\n",
    "          \"--ref_ver GRCh38 \" \\\n",
    "          \"--ref_fa {fasta} \" \\\n",
    "          \"--type {TE_type} \" \\\n",
    "          \"--chr ALL \" \\\n",
    "          \"--TSD_finding TRUE \" \\\n",
    "          \"--output {out_prefix} > {tmp_dir}{out_prefix}.log 2>&1\".format(\n",
    "        PALMER=PALMER, bam=bam, tmp_dir=outDir, fasta=hg38, TE_type=TE_type, out_prefix=out_prefix)\n",
    "    os.system(cmd)\n",
    "    \n",
    "/NAS/wg_looking/Tools/PALMER-1.7.2/PALMER --input canu_test1.correctedReads.bam \\\n",
    "    --workdir ./ \\\n",
    "    --ref_ver GRCh38 \\\n",
    "    --ref_fa /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa \\\n",
    "    --type ALU \\\n",
    "    --chr chr2 \\\n",
    "    --start 128000001 \\\n",
    "    --end 129000000 \\\n",
    "    --TSD_finding TRUE  \\\n",
    "    --output PALMER_run2 > PALMER_run2.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### local assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate medaka\n",
    "import os\n",
    "import shutil\n",
    "from multiprocessing import Pool\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "samclip = \"/NAS/wg_looking/Tools/samclip-0.4.0/samclip\"\n",
    "samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "\n",
    "\n",
    "def PrepareReads(TotalFastq, bam, Assembly_dir, region, AimFq):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    samtools = \"/NAS/wg_looking/Tools/samtools-1.11/samtools\"\n",
    "    aim_sam = Assembly_dir + \"/aim.sam\"\n",
    "    ReadsID_file = '%s/ReadsID.txt' % (Assembly_dir)\n",
    "    cmd = \"{samtools} view {bam} \\\"{region}\\\" > {aim_sam}\".format(\n",
    "            samtools=samtools, bam=bam, region=region, aim_sam=aim_sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"awk '{print $1}' %s |sort -u > %s\" % (aim_sam, ReadsID_file)\n",
    "    os.system(cmd)       \n",
    "    cmd1 = '{seqkit} grep --pattern-file {ReadsID_file} {TotalFq} > {outFastq}'.format(\n",
    "            seqkit=seqkit, ReadsID_file=ReadsID_file, TotalFq=TotalFastq, outFastq=AimFq)\n",
    "    print(cmd1)\n",
    "    os.system(cmd1)\n",
    "    return(AimFq)\n",
    "\n",
    "\n",
    "def shasta_run(fastq, ShastaRunDir):\n",
    "    shasta = \"/NAS/wg_looking/Tools/shasta_old/shasta-Linux-0.7.0\"\n",
    "    if fastq[-3:] == \".gz\":\n",
    "        print(\"Shasta load reads from uncompressed FASTA and FASTQ files !!\")\n",
    "    else:\n",
    "        cmd = \"{shasta} --MarkerGraph.minCoverage 5 \\\n",
    "         --Reads.minReadLength 300 \\\n",
    "        --input {reads} --assemblyDirectory {outdir}\".format(\n",
    "            shasta=shasta, reads=fastq, outdir=ShastaRunDir)\n",
    "        os.system(cmd)\n",
    "    return(os.path.join(ShastaRunDir, \"Assembly.fasta\"))\n",
    "\n",
    "\n",
    "def racon_pipeline(assemblyFa_init, reads, raconDir, ID, n_itera=4):\n",
    "    minimap2 = \"/NAS/wg_looking/Tools/minimap2-2.22_x64-linux/minimap2\"\n",
    "    ##\n",
    "    if not os.path.exists(raconDir):\n",
    "        os.mkdir(raconDir)\n",
    "    for i in range(n_itera):\n",
    "        if i == 0:\n",
    "            assemblyFa = assemblyFa_init\n",
    "        else:\n",
    "            before_i = i-1\n",
    "            assemblyFa = \"{raconDir}/{itera}x/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "                          raconDir=raconDir, name=ID, itera=before_i)\n",
    "        iteraDir = \"{raconDir}/{itera}x\".format(\n",
    "            raconDir=raconDir, itera=i)\n",
    "        if not os.path.exists(iteraDir):\n",
    "            os.mkdir(iteraDir)\n",
    "        outSam = \"{iteraDir}/{name}_VS_Assembly.sam\".format(\n",
    "            iteraDir=iteraDir, name=ID)\n",
    "        cmd = \"{minimap2} -a -t 3 -x map-ont -k 15 \" \\\n",
    "              \"{assemblyFa} {reads} > {outSam}\".format(\n",
    "              minimap2=minimap2, assemblyFa=assemblyFa, reads=reads, outSam=outSam)\n",
    "        os.system(cmd)\n",
    "        outFa = \"{iteraDir}/polished_racon_{name}_{itera}x.fasta\".format(\n",
    "            iteraDir=iteraDir, name=ID, itera=i)\n",
    "        cmd = \"racon -t 3 {reads} {outSam} {assemblyFa} \" \\\n",
    "              \"> {outFa}\".format(\n",
    "            reads=reads, outSam=outSam,\n",
    "            assemblyFa=assemblyFa, outFa=outFa)\n",
    "        os.system(cmd)\n",
    "    return(outFa)\n",
    "\n",
    "\n",
    "def medaka_run(assemblyFa, reads, medakaDir, ID):\n",
    "    cmd = \"medaka_consensus \\\n",
    "        -i {reads} \\\n",
    "        -d {assemblyFa} \\\n",
    "        -o {outDir} \\\n",
    "        -t 20 \\\n",
    "        -m r941_min_high_g360\".format(\n",
    "        reads=reads, assemblyFa=assemblyFa, outDir=medakaDir)\n",
    "    os.system(cmd)\n",
    "    #\n",
    "    cmd = \"medaka stitch \\\n",
    "        {medakaDir}/consensus_probs.hdf \\\n",
    "        {medakaDir}/consensus.fasta  {medakaDir}/{name}.assembly.fa\".format(\n",
    "        medakaDir=medakaDir, name=ID)\n",
    "    os.system(cmd)\n",
    "    return(\"{medakaDir}/{name}.assembly.fa\".format(medakaDir=medakaDir, name=ID))\n",
    "\n",
    "\n",
    "\n",
    "def AssemblyPIPE(TotalFastq, bam, outdir, region, regionName):\n",
    "    seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "    # identify dir names\n",
    "    Assembly_dir = os.path.join(outdir, regionName)\n",
    "    if not os.path.exists(Assembly_dir):\n",
    "        os.makedirs(Assembly_dir)\n",
    "    ShastaRunDir = os.path.join(Assembly_dir, 'Shasta')\n",
    "    raconDir = os.path.join(Assembly_dir, 'racon')\n",
    "    medakaDir = os.path.join(Assembly_dir, 'medaka')\n",
    "    #\n",
    "    AimFq = Assembly_dir + \"/aim.fastq\"\n",
    "    ShastaAssembly = os.path.join(ShastaRunDir, \"Assembly.fasta\")\n",
    "    raconAssembly = os.path.join(raconDir, \"3x/polished_racon_{name}_3x.fasta\".format(name=regionName))\n",
    "    ###\n",
    "    if not os.path.exists(AimFq) or os.path.getsize(AimFq)==0:\n",
    "        AimFq = PrepareReads(TotalFastq, bam, Assembly_dir, region, AimFq) \n",
    "    if not os.path.exists(ShastaAssembly):\n",
    "        if os.path.exists(ShastaRunDir):\n",
    "            shutil.rmtree(ShastaRunDir)\n",
    "        # shasta assembly\n",
    "        ShastaAssembly = shasta_run(AimFq, ShastaRunDir) \n",
    "    if os.path.exists(ShastaAssembly):\n",
    "        if os.path.getsize(ShastaAssembly)==0:\n",
    "            shutil.rmtree(ShastaRunDir)\n",
    "            ShastaAssembly = shasta_run(AimFq, ShastaRunDir) \n",
    "    ###\n",
    "    if not os.path.exists(raconAssembly):  \n",
    "        if os.path.exists(raconDir):\n",
    "            shutil.rmtree(raconDir)\n",
    "        # racon assembly use shasta Assembly as init assembly\n",
    "        raconAssembly = racon_pipeline(ShastaAssembly, AimFq, raconDir, regionName)  \n",
    "    if os.path.exists(raconAssembly):\n",
    "        if os.path.getsize(raconAssembly)==0:\n",
    "            shutil.rmtree(raconDir)\n",
    "            raconAssembly = racon_pipeline(ShastaAssembly, AimFq, raconDir, regionName)   \n",
    "    # medaka consensus use racon Assembly as input\n",
    "    if os.path.getsize(raconAssembly):\n",
    "        if os.path.exists(medakaDir):\n",
    "            shutil.rmtree(medakaDir)\n",
    "        medakaAssembly = medaka_run(raconAssembly, AimFq, medakaDir, regionName)\n",
    "        # convert final assembly to tab file\n",
    "        if os.path.exists(medakaAssembly):\n",
    "            os.system('{seqkit} fx2tab -i {medakaAssembly} > {medakaAssembly}.tab'.format(\n",
    "                seqkit=seqkit, medakaAssembly=medakaAssembly))\n",
    "\n",
    "        \n",
    "def main(TotalFastq, ID, bam, TE_calls, outdir, AssemblyFasta): \n",
    "    # \n",
    "    with open(TE_calls, \"r\") as fin:\n",
    "        pool = Pool(5)\n",
    "        for line in fin:\n",
    "            info = line.split()\n",
    "            if info[0] == \"cluster_id\":\n",
    "                continue\n",
    "            readSupport = int(info[11])\n",
    "            if readSupport >= 3: \n",
    "                chrom = info[1]\n",
    "                start1 = int(info[2])\n",
    "                start2 = int(info[3])\n",
    "                start = min([start1, start2]) - 500\n",
    "                end = max([start1, start2]) + 500\n",
    "                region = chrom + \":\" + str(start) + \"-\" + str(end)\n",
    "                regionName = chrom + \"_\" + str(start1) + \"_\" + str(start2)\n",
    "                medakaAssembly = os.path.join(outdir, regionName, 'medaka/{name}.assembly.fa.tab'.format(\n",
    "                                        name=regionName))\n",
    "                if not os.path.exists(medakaAssembly):\n",
    "                    try:\n",
    "                        AssemblyPIPE(TotalFastq, bam, outdir, region, regionName)\n",
    "                        # pool.apply_async(AssemblyPIPE, args=(TotalFastq, bam, outdir, region, regionName,))\n",
    "                    except:\n",
    "                        pass\n",
    "                        continue\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        del pool\n",
    "    ##  out fasta\n",
    "    with open(AssemblyFasta, \"w\") as OutPut:\n",
    "        for Name in os.listdir(outdir):\n",
    "            medakaAssembly = os.path.join(outdir, Name, 'medaka/{name}.assembly.fa.tab'.format(name=Name))\n",
    "            if os.path.exists(medakaAssembly):\n",
    "                with open(medakaAssembly, 'r') as fpin:\n",
    "                    for records in fpin.readlines():\n",
    "                        info = records.strip().split(\"\\t\")\n",
    "                        ASMID = '{Name}_{ID}'.format(Name=Name, ID=info[0])\n",
    "                        Seq = info[1]\n",
    "                        if len(Seq) > 500: # we only need assembly longer than 500 bp\n",
    "                            OutPut.write('>{ASMID}\\n{Seq}\\n'.format(ASMID=ASMID, Seq=Seq))\n",
    "                            OutPut.flush()\n",
    "\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda activate medaka\n",
    "\n",
    "import os\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "ID_dict = {\"sgRNA_HCT116\":['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3'],\n",
    "           \"totalRNA_HCT116\":[\"totalRNA_HCT116\"],\n",
    "           \"sgRNA_NCM460\":['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3'],\n",
    "           \"totalRNA_NCM460\":['totalRNA_NCM460', 'totalRNA_NCM460_2'],\n",
    "           \"sgRNA_K562_1\":[\"sgRNA_K562_1\"],\n",
    "           \"totalRNA_K562\":['totalRNA_K562_1', 'totalRNA_K562_2', 'totalRNA_K562_3'],\n",
    "           \"sgRNA_MDAMB231\":['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3'],\n",
    "           \"sgRNA_MCF10A\":['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3'] \n",
    "          }\n",
    "\n",
    "for SeqCell in ID_dict.keys()：\n",
    "    for TE in [\"ALU\", \"LINE\"]:\n",
    "        TE_calls = work_dir + \"/merge_PALMER/%s/%s/%s.PALMER.%s_calls.txt\" % (SeqCell, TE, SeqCell, TE)\n",
    "        bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (SeqCell, SeqCell)\n",
    "        TotalFastq = work_dir + \"/QC/%s.trimming.Q7.L300.fq.gz\" % (SeqCell) \n",
    "        outdir = work_dir + \"/PALMER_non_ref_TE/%s/%s\" % (SeqCell, TE)\n",
    "        AssemblyFasta = outdir + '/%s_%s_localAssembly.fa' % (SeqCell, TE)  \n",
    "        main(TotalFastq, SeqCell, bam, TE_calls, outdir, AssemblyFasta)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mapping local Assembly fasta to reference\n",
    "import re\n",
    "cuteSV = '/NAS/wg_looking/Tools/cuteSV-cuteSV-v1.0.12/src/cuteSV/cuteSV'\n",
    "work_dir = '/NAS/wg_looking/gRNA_ONT'\n",
    "ID = \"sgRNA_K562_1\"\n",
    "\n",
    "\n",
    "for ID in fastq_dict.keys():\n",
    "    AssemblyFasta = work_dir + '/PALMER_non_ref_TE/%s/ALU/%s_ALU_localAssembly.fa' % (ID, ID)\n",
    "    sam = work_dir + '/PALMER_non_ref_TE/%s/ALU/%s_localAssembly.sam' % (ID, ID)\n",
    "    bam = work_dir + '/PALMER_non_ref_TE/%s/ALU/%s_localAssembly.sort.bam' % (ID, ID)\n",
    "    minimap2_run(hg38, AssemblyFasta, bam)\n",
    "    #\n",
    "    outINS = work_dir + '/PALMER_non_ref_TE/%s/ALU/%s_localAssembly_INS_info.bed' % (ID, ID)\n",
    "    with open(outINS, \"w\") as fout:\n",
    "        # fout.write(\"\\t\".join([\"INS_name\", \"INS_start\", \"INS_end\", \"chrom\", \"INS_pos\"]) + \"\\n\")\n",
    "        with open(sam, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                if line[0] == \"@\":\n",
    "                    continue\n",
    "                info = line.split()\n",
    "                INS_name = info[0]\n",
    "                Flag = int(info[1])\n",
    "                chrom = info[2]\n",
    "                tmp_INS_pos = int(info[3])\n",
    "                match_info = info[5]\n",
    "                contig_length = len(info[9])\n",
    "                if int(info[4])==60:\n",
    "                    match = re.sub( r\"([A-Z])\", r\"\\1 \", match_info).split()\n",
    "                    INS_index = [i for i in range(0, len(match)) if re.search(r'I', match[i])]\n",
    "                    H_index = [i for i in range(0, len(match)) if re.search(r'H', match[i])]\n",
    "                    for i in H_index:\n",
    "                        H_length = int(match[i][:-1])\n",
    "                        contig_length = contig_length + H_length\n",
    "                    for i in INS_index:\n",
    "                        INS_length = int(match[i][:-1])\n",
    "                        if INS_length > 50:\n",
    "                            print(i)\n",
    "                            read_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"D\" or m[-1] == \"N\":\n",
    "                                    continue\n",
    "                                read_location = read_location + int(m[:-1])\n",
    "                            ref_location = 0\n",
    "                            for m in match[:i]:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                ref_location = ref_location + int(m[:-1])\n",
    "                            end_location = 0\n",
    "                            for m in match:\n",
    "                                if m[-1] == \"H\" or m[-1] == \"S\" or m[-1] == \"I\":\n",
    "                                    continue\n",
    "                                end_location = end_location + int(m[:-1])\n",
    "                            ###   \n",
    "                            if Flag == 0 or Flag == 2048:\n",
    "                                INS_start = read_location\n",
    "                                INS_end = read_location + INS_length\n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            if Flag == 16 or Flag == 2064:\n",
    "                                INS_start = contig_length - read_location - INS_length\n",
    "                                INS_end = contig_length - read_location \n",
    "                                INS_pos = tmp_INS_pos + ref_location\n",
    "                            outinfo = [INS_name, str(INS_start), str(INS_end), chrom, str(INS_pos)]\n",
    "                            fout.write(\"\\t\".join(outinfo) + \"\\n\")\n",
    "    #\n",
    "    # os.chdir(work_dir + '/%s' % ID)\n",
    "    # outVCF = work_dir + '/%s/%s.softClipped.vcf' % (ID, ID)\n",
    "    # cmd = \"{cuteSV} --threads 50 \\\n",
    "    #         --report_readid \\\n",
    "    #         --min_read_len 300 \\\n",
    "    #         --max_split_parts -1 \\\n",
    "    #         --merge_ins_threshold 100 \\\n",
    "    #         --min_support 1 \\\n",
    "    #         --genotype \\\n",
    "    #         --min_size 50 \\\n",
    "    #         --diff_ratio_merging_INS 0.3 \\\n",
    "    #         --max_cluster_bias_INS 100 \\\n",
    "    #         --max_cluster_bias_DEL 100 \\\n",
    "    #         --diff_ratio_merging_DEL 0.3 \\\n",
    "    #         --sample {sampleID} \\\n",
    "    #         {sort_bam} {reference} {outVCF} {work_dir}\".format(\n",
    "    #     cuteSV=cuteSV, sampleID=ID, sort_bam=bam, reference=hg38, outVCF=outVCF, work_dir=\"./\")\n",
    "    # os.system(cmd)\n",
    "    ## run RepeatMasker on local Assembly fasta\n",
    "    outDir = work_dir + \"/\" + ID \n",
    "    cmd = \"{RepeatMasker} -pa 100 -a -species human {fa} -dir {outDir}\".format(\n",
    "        RepeatMasker=RepeatMasker, fa=AssemblyFasta, outDir=outDir)\n",
    "    os.system(cmd)\n",
    "    ##\n",
    "    cmd = \"awk '{OFS=\\\"\\t\\\"} $1>=225 && $2 <=18 && ($11==\\\"SINE/Alu\\\" || $11==\\\"LINE/L1\\\") {print $5,$6,$7,$10}' \\\n",
    "            %s.out > %s.RM.bed\" % (AssemblyFasta, AssemblyFasta[:-3])\n",
    "    os.system(cmd)\n",
    "    outBed = work_dir + '/%s/%s_softClipped_localAssembly_INS_TE.bed' % (ID, ID)\n",
    "    cmd = \"{bedtools} intersect -a {INS_bed} -b {prefix}.RM.bed -wa -wb > {out}\".format(\n",
    "        bedtools=bedtools, INS_bed=outINS, prefix=AssemblyFasta[:-3], out=outBed)\n",
    "    os.system(cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awk '$12>=3 {print $0}' */PALMER_res/LINE/*.PALMER.LINE_calls.txt |less -S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"tldr -b sgRNA_K562_1.Q7.L300.sort.bam \\\n",
    "-e /NAS/wg_looking/Tools/tldr-1.2.2/ref/teref.ont.human.fa \\\n",
    "-r /NAS/wg_looking/human_ref/GRCh38.p13.genome.fa \\\n",
    "--color_consensus -p 50 --minreads 10 --outbase sgRNA_K562_1.tldr\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "# StringTie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## StingTie run and gffcompare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameter: minCoverge, gap=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "## Samples\n",
    "Samples_dict = {\"HCT116\": {\"totalRNA\": ['totalRNA_HCT116'], \n",
    "                           \"sgRNA\": ['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3']},\n",
    "                \"NCM460\": {\"totalRNA\": ['totalRNA_NCM460', \"totalRNA_NCM460_2\"],\n",
    "                           \"sgRNA\": ['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3']},\n",
    "                \"K562\": {\"totalRNA\": [\"totalRNA_K562_1\", \"totalRNA_K562_2\", \"totalRNA_K562_3\"],\n",
    "                         \"sgRNA\": ['sgRNA_K562_1']},\n",
    "                \"MDAMB231\": {\"sgRNA\": ['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3']} ,\n",
    "                \"MCF10A\": {\"sgRNA\": ['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3']} \n",
    "               }\n",
    "\n",
    "\n",
    "def StingTie_run(bams, ref_gtf, out_gtf, minCoverage, multiMin=1, singleMin=4.75):\n",
    "    # cmd = \"{stringtie} -L -p 50 -j 3 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "    #     stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=bam, multiMin=multiMin, singleMin=singleMin)\n",
    "    # os.system(cmd)\n",
    "    # cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "    # stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=bam, multiMin=multiMin, singleMin=singleMin)\n",
    "    cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "        stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=\" \".join(bams), multiMin=minCoverage, singleMin=minCoverage)\n",
    "    os.system(cmd)  \n",
    "    cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input}\".format(\n",
    "        gffcompare=gffcompare, ref_gtf=ref_gtf, outprefix=out_gtf[:-4], input=out_gtf)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# minCoverage\n",
    "pool = Pool(3)\n",
    "for CellLine in Samples_dict.keys():\n",
    "    for SeqType in Samples_dict[CellLine].keys():\n",
    "        IDs = Samples_dict[CellLine][SeqType]\n",
    "        bams = []\n",
    "        for ID in IDs:\n",
    "            bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "            bams.append(bam) \n",
    "        stringtie_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "        if not os.path.exists(stringtie_dir):\n",
    "            os.makedirs(stringtie_dir)\n",
    "        for minCoverage  in range(1, 20): \n",
    "            out_gtf = stringtie_dir + \"/%s.minCoverage_%s.gtf\" % (SeqType + \"_\" + CellLine, minCoverage)\n",
    "            pool.apply_async(StingTie_run, args=(bams, ref_gtf, out_gtf, minCoverage,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max gap , coverage >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "## Samples\n",
    "Samples_dict = {\"HCT116\": {\"totalRNA\": ['totalRNA_HCT116'], \n",
    "                           \"sgRNA\": ['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3']},\n",
    "                \"NCM460\": {\"totalRNA\": ['totalRNA_NCM460', \"totalRNA_NCM460_2\"],\n",
    "                           \"sgRNA\": ['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3']},\n",
    "                \"K562\": {\"totalRNA\": [\"totalRNA_K562_1\", \"totalRNA_K562_2\", \"totalRNA_K562_3\"],\n",
    "                         \"sgRNA\": ['sgRNA_K562_1']},\n",
    "                \"MDAMB231\": {\"sgRNA\": ['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3']} ,\n",
    "                \"MCF10A\": {\"sgRNA\": ['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3']} \n",
    "               }\n",
    "\n",
    "\n",
    "def StingTie_run(bams, ref_gtf, out_gtf, minCoverage, gap):\n",
    "    cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam} -g {gap}\".format(\n",
    "        stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=\" \".join(bams), \n",
    "        multiMin=minCoverage, singleMin=minCoverage, gap=gap)\n",
    "    os.system(cmd)  \n",
    "    cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input}\".format(\n",
    "        gffcompare=gffcompare, ref_gtf=ref_gtf, outprefix=out_gtf[:-4], input=out_gtf)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# minCoverage\n",
    "pool = Pool(3)\n",
    "for CellLine in Samples_dict.keys():\n",
    "    for SeqType in Samples_dict[CellLine].keys():\n",
    "        IDs = Samples_dict[CellLine][SeqType]\n",
    "        bams = []\n",
    "        for ID in IDs:\n",
    "            bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "            bams.append(bam) \n",
    "        stringtie_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "        if not os.path.exists(stringtie_dir):\n",
    "            os.makedirs(stringtie_dir)\n",
    "        for gap in range(0, 55, 5): \n",
    "            minCoverage = 5\n",
    "            out_gtf = stringtie_dir + \"/%s.minCoverage_%s.gap_%s.gtf\" % (SeqType + \"_\" + CellLine, minCoverage, gap)\n",
    "            if not os.path.exists(out_gtf):\n",
    "                pool.apply_async(StingTie_run, args=(bams, ref_gtf, out_gtf, minCoverage, gap))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### minCoverage, gap=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "## Samples\n",
    "Samples_dict = {\"HCT116\": {\"totalRNA\": ['totalRNA_HCT116'], \n",
    "                           \"sgRNA\": ['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3']},\n",
    "                \"NCM460\": {\"totalRNA\": ['totalRNA_NCM460', \"totalRNA_NCM460_2\"],\n",
    "                           \"sgRNA\": ['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3']},\n",
    "                \"K562\": {\"totalRNA\": [\"totalRNA_K562_1\", \"totalRNA_K562_2\", \"totalRNA_K562_3\"],\n",
    "                         \"sgRNA\": ['sgRNA_K562_1']},\n",
    "                \"MDAMB231\": {\"sgRNA\": ['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3']} ,\n",
    "                \"MCF10A\": {\"sgRNA\": ['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3']} \n",
    "               }\n",
    "\n",
    "\n",
    "def StingTie_run(bams, ref_gtf, out_gtf, minCoverage):\n",
    "    # cmd = \"{stringtie} -L -p 50 -j 3 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "    #     stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=bam, multiMin=multiMin, singleMin=singleMin)\n",
    "    # os.system(cmd)\n",
    "    # cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "    # stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=bam, multiMin=multiMin, singleMin=singleMin)\n",
    "    cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -G {gtf} -o {outgtf} {bam} -g 20\".format(\n",
    "        stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=\" \".join(bams), \n",
    "        multiMin=minCoverage, singleMin=minCoverage)\n",
    "    os.system(cmd)  \n",
    "    cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input}\".format(\n",
    "        gffcompare=gffcompare, ref_gtf=ref_gtf, outprefix=out_gtf[:-4], input=out_gtf)\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# minCoverage\n",
    "pool = Pool(5)\n",
    "for CellLine in Samples_dict.keys():\n",
    "    for SeqType in Samples_dict[CellLine].keys():\n",
    "        IDs = Samples_dict[CellLine][SeqType]\n",
    "        bams = []\n",
    "        for ID in IDs:\n",
    "            bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "            bams.append(bam) \n",
    "        stringtie_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "        if not os.path.exists(stringtie_dir):\n",
    "            os.makedirs(stringtie_dir)\n",
    "        for minCoverage  in range(1, 20):  \n",
    "            out_gtf = stringtie_dir + \"/%s.minCoverage_%s.gap_20.gtf\" % (SeqType + \"_\" + CellLine, minCoverage)\n",
    "            if not os.path.exists(out_gtf):\n",
    "                pool.apply_async(StingTie_run, args=(bams, ref_gtf, out_gtf, minCoverage,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HCT116 + NCM460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "sgIDs_dict = dict(HCT116=['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3'],\n",
    "                  NCM460=['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3'])\n",
    "\n",
    "\n",
    "## bam\n",
    "bams = []\n",
    "for CellLine in sgIDs_dict.keys():\n",
    "    for ID in sgIDs_dict[CellLine]:\n",
    "        bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "        bams.append(bam) \n",
    "\n",
    "##\n",
    "outDir = work_dir + \"/stringtie_res5/HCT116_NCM460\"\n",
    "if not os.path.exists(outDir):\n",
    "    os.makedirs(outDir)\n",
    "\n",
    "##\n",
    "sg_merge_gtf = outDir + \"/sgRNA_HCT116_NCM460.minCoverage_5.gap_20.gtf\" \n",
    "cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -g {gap} -G {gtf} -o {outgtf} {bam}\".format(\n",
    "    stringtie=stringtie, gtf=ref_gtf, outgtf=sg_merge_gtf, bam=\" \".join(bams), multiMin=5, singleMin=5, gap=20)\n",
    "os.system(cmd)  \n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input}\".format(\n",
    "    gffcompare=gffcompare, ref_gtf=ref_gtf, outprefix=sg_merge_gtf[:-4], input=sg_merge_gtf)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "##\n",
    "os.chdir(outDir)\n",
    "sg_HCT116_NCM460 = outDir + \"/sgRNA_HCT116_NCM460.minCoverage_5.gap_20.gtf\" \n",
    "sgHCT116 = work_dir + \"/stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "sgNCM460 = work_dir + \"/stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\"\n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=sg_HCT116_NCM460, outprefix=\"sg_HCT116_NCM460_vs_sgHCT116\", input=sgHCT116)\n",
    "os.system(cmd)\n",
    "\n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=sg_HCT116_NCM460, outprefix=\"sg_HCT116_NCM460_vs_sgNCM460\", input=sgNCM460)\n",
    "os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### totalRNA_K562_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\"\n",
    "gffcompare = \"/NAS/wg_looking/Tools/gffcompare-0.12.6/gffcompare\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "## Samples\n",
    "Samples_dict = {\n",
    "                \"K562\": {\"totalRNA\": [\"totalRNA_K562_1\"]}\n",
    "               }\n",
    "\n",
    "    \n",
    "# minCoverage=5, gap=20\n",
    "for CellLine in Samples_dict.keys():\n",
    "    for SeqType in Samples_dict[CellLine].keys():\n",
    "        IDs = Samples_dict[CellLine][SeqType]\n",
    "        bams = []\n",
    "        for ID in IDs:\n",
    "            bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "            bams.append(bam) \n",
    "        stringtie_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "        if not os.path.exists(stringtie_dir):\n",
    "            os.makedirs(stringtie_dir)\n",
    "        out_gtf = stringtie_dir + \"/%s.minCoverage_5.gap_20.gtf\" % (ID)\n",
    "        cmd = \"{stringtie} -L -p 50 -c {multiMin} -s {singleMin} -g 20 -G {gtf} -o {outgtf} {bam}\".format(\n",
    "        stringtie=stringtie, gtf=ref_gtf, outgtf=out_gtf, bam=\" \".join(bams), multiMin=5, singleMin=5)\n",
    "        os.system(cmd)  \n",
    "        cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input}\".format(\n",
    "            gffcompare=gffcompare, ref_gtf=ref_gtf, outprefix=out_gtf[:-4], input=out_gtf)\n",
    "        os.system(cmd)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALL samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_and_gffcomp(minCoverage, minFrac, gtfs, merge_gtf):\n",
    "    gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "    cmd = \"{stringtie} --merge {gtf_list} -G {gtf} -o {merge_gtf} -m 50 -c {min_cov} -f {min_frac} -l ALLSTRG\".format(\n",
    "        stringtie=stringtie, gtf_list=\" \".join(gtfs), gtf=gtf, merge_gtf=merge_gtf,\n",
    "        min_cov=minCoverage, min_frac=minFrac)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{gffcompare} -r {hg38_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, hg38_gtf=gtf, outprefix=merge_gtf[:-4], input=merge_gtf)\n",
    "    os.system(cmd)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "IDs_dict = dict(HCT116=['sgRNA_HCT116_1', 'sgRNA_HCT116_2', 'sgRNA_HCT116_3', 'totalRNA_HCT116'],\n",
    "                NCM460=['sgRNA_NCM460_1', 'sgRNA_NCM460_2', 'sgRNA_NCM460_3', 'totalRNA_NCM460', \"totalRNA_NCM460_2\"],\n",
    "                K562=['sgRNA_K562_1', \"totalRNA_K562_1\", \"totalRNA_K562_2\", \"totalRNA_K562_3\"],\n",
    "                MDAMB231=['sgRNA_MDAMB231_1', 'sgRNA_MDAMB231_2', 'sgRNA_MDAMB231_3'],\n",
    "                MCF10A=['sgRNA_MCF10A_1', 'sgRNA_MCF10A_2', 'sgRNA_MCF10A_3'])\n",
    "   \n",
    "    \n",
    "gtfs = []\n",
    "for CellLine in IDs_dict.keys():\n",
    "    for ID in IDs_dict[CellLine]:\n",
    "        out_gtf = work_dir + \"/stringtie_res2/%s/%s.multiMin_%s.singleMin_%s.stringtie.gtf\" % (CellLine, ID, 1, 3)\n",
    "        gtfs.append(out_gtf)\n",
    "\n",
    "\n",
    "        \n",
    "# All\n",
    "pool = Pool(10)\n",
    "for minCoverage  in range(1, 20): \n",
    "    for minFrac in range(5, 100, 5):\n",
    "        minFrac = minFrac/100\n",
    "        outDir = work_dir + \"/stringtie_res2/ALL\"\n",
    "        if not os.path.exists(outDir):\n",
    "            os.makedirs(outDir)\n",
    "        merge_gtf = outDir + \"/ALL.minCoverage_%s.minFrac_%s.stringtie.gtf\" % (\n",
    "             minCoverage, minFrac)\n",
    "        pool.apply_async(merge_and_gffcomp, args=(minCoverage, minFrac, gtfs, merge_gtf))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistic\n",
    "import os\n",
    "\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "IDs = [\"sgRNA_HCT116\", \"totalRNA_HCT116\", \"sgRNA_NCM460\", \"totalRNA_NCM460\", \n",
    "       \"totalRNA_K562\", \"sgRNA_K562\", \"sgRNA_MDAMB231\", \"sgRNA_MCF10A\"]\n",
    "# IDs = [\"sgRNA_HCT116_NCM460\"]\n",
    "\n",
    "# minCoverage\n",
    "for ID in IDs:\n",
    "    CellLine = ID.split(\"_\")[1]\n",
    "    stat_for_plot_1 = work_dir + \"/stringtie_res5/%s.gffcmp.stats_sensitivity_precision.txt\" % (ID)\n",
    "    stat1 = open(stat_for_plot_1, \"w\")\n",
    "    stat1.write(\"minCoverage\\tType\\tstats\\tPercent\\n\")\n",
    "    for minCoverage  in range(1, 20):\n",
    "        statFile = work_dir + \"/stringtie_res5/%s/%s.minCoverage_%s\" % (\n",
    "            CellLine, ID, minCoverage)\n",
    "        gffcmp_stat = open(statFile, \"r\")\n",
    "        line = gffcmp_stat.readline()\n",
    "        while line:\n",
    "            if line[0] == \"#\" or line == \"\\n\":\n",
    "                line = gffcmp_stat.readline()\n",
    "                continue\n",
    "            info = line.split()\n",
    "            if info[1][:-1] == \"level\":\n",
    "                Type = info[0] + \"_\" + info[1][:-1]\n",
    "                Sensitivity = info[2]\n",
    "                Precision = info[4]\n",
    "                F1 = 2*float(Sensitivity)*float(Precision)/(float(Sensitivity) + float(Precision))\n",
    "                stat1.write(\"\\t\".join([str(minCoverage), Type, \"Sensitivity\", Sensitivity]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minCoverage), Type, \"Precision\", Precision]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minCoverage), Type, \"F1_score\", str(F1)]) + \"\\n\")\n",
    "                stat1.flush()\n",
    "            line = gffcmp_stat.readline()\n",
    "        gffcmp_stat.close()\n",
    "    stat1.close()\n",
    "\n",
    "    \n",
    "# gap\n",
    "for ID in IDs:\n",
    "    CellLine = ID.split(\"_\")[1]\n",
    "    stat_for_plot_1 = work_dir + \"/stringtie_res5/%s.gap.gffcmp.stats_sensitivity_precision.txt\" % (ID)\n",
    "    stat1 = open(stat_for_plot_1, \"w\")\n",
    "    stat1.write(\"max_gap\\tType\\tstats\\tPercent\\n\")\n",
    "    for gap  in range(0, 30, 5):\n",
    "        statFile = work_dir + \"/stringtie_res5/%s/%s.minCoverage_5.gap_%s\" % (\n",
    "            CellLine, ID, gap)\n",
    "        gffcmp_stat = open(statFile, \"r\")\n",
    "        line = gffcmp_stat.readline()\n",
    "        while line:\n",
    "            if line[0] == \"#\" or line == \"\\n\":\n",
    "                line = gffcmp_stat.readline()\n",
    "                continue\n",
    "            info = line.split()\n",
    "            if info[1][:-1] == \"level\":\n",
    "                Type = info[0] + \"_\" + info[1][:-1]\n",
    "                Sensitivity = info[2]\n",
    "                Precision = info[4]\n",
    "                F1 = 2*float(Sensitivity)*float(Precision)/(float(Sensitivity) + float(Precision))\n",
    "                stat1.write(\"\\t\".join([str(gap), Type, \"Sensitivity\", Sensitivity]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(gap), Type, \"Precision\", Precision]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(gap), Type, \"F1_score\", str(F1)]) + \"\\n\")\n",
    "                stat1.flush()\n",
    "            line = gffcmp_stat.readline()\n",
    "        gffcmp_stat.close()\n",
    "    stat1.close()\n",
    "\n",
    "    \n",
    "#  minFrac\n",
    "for ID in IDs:\n",
    "    CellLine = ID.split(\"_\")[1]\n",
    "    stat_for_plot_1 = work_dir + \"/stringtie_res4/%s.minFrac.gffcmp.stats_sensitivity_precision.txt\" % (ID)\n",
    "    stat1 = open(stat_for_plot_1, \"w\")\n",
    "    stat1.write(\"minFrac\\tType\\tstats\\tPercent\\n\")\n",
    "    for minFrac in range(5, 100, 5):\n",
    "        minFrac = minFrac/100\n",
    "        statFile = work_dir + \"/stringtie_res4/%s/%s.minFrac_%s.stringtie\" % (\n",
    "            CellLine, ID, minFrac)\n",
    "        gffcmp_stat = open(statFile, \"r\")\n",
    "        line = gffcmp_stat.readline()\n",
    "        while line:\n",
    "            if line[0] == \"#\" or line == \"\\n\":\n",
    "                line = gffcmp_stat.readline()\n",
    "                continue\n",
    "            info = line.split()\n",
    "            if info[1][:-1] == \"level\":\n",
    "                Type = info[0] + \"_\" + info[1][:-1]\n",
    "                Sensitivity = info[2]\n",
    "                Precision = info[4]\n",
    "                F1 = 2*float(Sensitivity)*float(Precision)/(float(Sensitivity) + float(Precision))\n",
    "                stat1.write(\"\\t\".join([str(minFrac), Type, \"Sensitivity\", Sensitivity]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minFrac), Type, \"Precision\", Precision]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minFrac), Type, \"F1_score\", str(F1)]) + \"\\n\")\n",
    "                stat1.flush()\n",
    "            line = gffcmp_stat.readline()\n",
    "        gffcmp_stat.close()\n",
    "    stat1.close()\n",
    "\n",
    "\n",
    "\n",
    "#  minFPKM\n",
    "for ID in IDs:\n",
    "    CellLine = ID.split(\"_\")[1]\n",
    "    stat_for_plot_1 = work_dir + \"/stringtie_res4/%s.minFPKM.gffcmp.stats_sensitivity_precision.txt\" % (ID)\n",
    "    stat1 = open(stat_for_plot_1, \"w\")\n",
    "    stat1.write(\"minFPKM\\tType\\tstats\\tPercent\\n\")\n",
    "    for minFPKM in range(5, 100, 5):\n",
    "        minFPKM = minFPKM/10\n",
    "        statFile = work_dir + \"/stringtie_res4/%s/%s.minFPKM_%s.stringtie\" % (\n",
    "            CellLine, ID, minFPKM)\n",
    "        gffcmp_stat = open(statFile, \"r\")\n",
    "        line = gffcmp_stat.readline()\n",
    "        while line:\n",
    "            if line[0] == \"#\" or line == \"\\n\":\n",
    "                line = gffcmp_stat.readline()\n",
    "                continue\n",
    "            info = line.split()\n",
    "            if info[1][:-1] == \"level\":\n",
    "                Type = info[0] + \"_\" + info[1][:-1]\n",
    "                Sensitivity = info[2]\n",
    "                Precision = info[4]\n",
    "                F1 = 2*float(Sensitivity)*float(Precision)/(float(Sensitivity) + float(Precision))\n",
    "                stat1.write(\"\\t\".join([str(minFPKM), Type, \"Sensitivity\", Sensitivity]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minFPKM), Type, \"Precision\", Precision]) + \"\\n\")\n",
    "                stat1.write(\"\\t\".join([str(minFPKM), Type, \"F1_score\", str(F1)]) + \"\\n\")\n",
    "                stat1.flush()\n",
    "            line = gffcmp_stat.readline()\n",
    "        gffcmp_stat.close()\n",
    "    stat1.close()\n",
    "\n",
    "\n",
    "\n",
    "# confirm final gtf with statistical results from R plot\n",
    "# for K562\n",
    "sgK562 = work_dir + \"/stringtie_res5/K562/sgRNA_K562.minCoverage_5.gap_20.gtf\"\n",
    "totalK562 = work_dir + \"/stringtie_res5/K562/totalRNA_K562.minCoverage_5.gap_20.gtf\"\n",
    "totalK562_1 = work_dir + \"/stringtie_res5/K562/totalRNA_K562_1.minCoverage_5.gap_20.gtf\"\n",
    "# for HCT116\n",
    "sgHCT116 = work_dir + \"/stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "totalHCT116 = work_dir + \"/stringtie_res5/HCT116/totalRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "# for NCM460\n",
    "sgNCM460 = work_dir + \"/stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\"\n",
    "totalNCM460 = work_dir + \"/stringtie_res5/NCM460/totalRNA_NCM460.minCoverage_5.gap_20.gtf\"  \n",
    "# for MDAMB231\n",
    "sgMDAMB231 = work_dir + \"/stringtie_res5/MDAMB231/sgRNA_MDAMB231.minCoverage_5.gap_20.gtf\"\n",
    "# MCF10A\n",
    "sgMCF10A = work_dir + \"/stringtie_res5/MCF10A/sgRNA_MCF10A.minCoverage_5.gap_20.gtf\"\n",
    "# HCT116 + NCM460\n",
    "# sg_HCT116_NCM460 = work_dir + \"/stringtie_res4/HCT116_NCM460/sgRNA_HCT116_NCM460.minCoverage_13.stringtie.gtf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new transcriptsL: new splicing junction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_14460/2124147100.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\ADMINI~1\\AppData\\Local\\Temp/ipykernel_14460/2124147100.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    NAS/wg_looking/Tools/pygtftk-1.5.3/bin/gtftk(intronic, -i, gencode.v37.chr_patch_hapl_scaff.annotation.gtf, --by-transcript, -o, gencode.v37.chr_patch_hapl_scaff.annotation.intron.bed)\u001b[0m\n\u001b[1;37m                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# conda activate pygtftk\n",
    "import os\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "hg38_intron_bed = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.intron.bed\"\n",
    "hg38_intron_bed_modif = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.intron.modif.bed\"\n",
    "\n",
    "\n",
    "# /NAS/wg_looking/Tools/pygtftk-1.5.3/bin/gtftk intronic -i gencode.v37.chr_patch_hapl_scaff.annotation.gtf \\\n",
    "# --by-transcript \\\n",
    "# -o gencode.v37.chr_patch_hapl_scaff.annotation.intron.bed\n",
    "\n",
    "def get_new_intron_bed(assembly_gtf):\n",
    "    intron_bed = assembly_gtf[:-3] + \"intron.bed\"\n",
    "    intron_bed_ref = assembly_gtf[:-3] + \"intron.modif.ref.bed\"\n",
    "    new_intron_bed = assembly_gtf[:-3] + \"intron.new.bed\"\n",
    "    if not os.path.exists(intron_bed):\n",
    "        cmd = \"/NAS/wg_looking/Tools/pygtftk-1.5.3/bin/gtftk intronic \\\n",
    "              -i {gtf} --by-transcript -o {intron_bed}\".format(\n",
    "            gtf=assembly_gtf, intron_bed=intron_bed)\n",
    "        os.system(cmd)\n",
    "    basename = os.path.basename(assembly_gtf)\n",
    "    tmap = assembly_gtf.split(\"annotated.gtf\")[0] + basename.split(\"annotated.gtf\")[0] + \"gtf.tmap\"\n",
    "    if os.path.exists(tmap):\n",
    "        tmap_df = pd.read_csv(tmap, sep=\"\\t\")\n",
    "        tmap_df = tmap_df.loc[:, [\"ref_id\", \"qry_id\"]]\n",
    "        intron_df = pd.read_csv(intron_bed, sep=\"\\t\", header=None)\n",
    "        qry_id = [x.split(\"|\")[2] for x in intron_df[3]]\n",
    "        intron_df['qry_id'] = qry_id\n",
    "        intron_df = intron_df.iloc[:, [1,2,6]]\n",
    "        intron_df.columns = [\"start\", \"end\", \"qry_id\"]\n",
    "        merge = pd.merge(intron_df, tmap_df, on=\"qry_id\")\n",
    "        merge = merge.loc[:, [\"ref_id\", \"start\", \"end\", \"qry_id\"]]\n",
    "        merge.to_csv(intron_bed_ref, index=False, sep=\"\\t\", header=False)\n",
    "        cmd = \"/NAS/wg_looking/Tools/bedtools2/bin/bedtools intersect \\\n",
    "               -a {intron_bed} \\\n",
    "               -b {hg38_intron_bed} -v -f 0.99 -F 0.99  > \\\n",
    "               {new_intron_bed}\".format(\n",
    "        intron_bed=intron_bed_ref, hg38_intron_bed=hg38_intron_bed_modif, new_intron_bed=new_intron_bed)\n",
    "        os.system(cmd)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "for CellLine in [\"HCT116\", \"K562\", \"MCF10A\", \"MDAMB231\", \"NCM460\"]:\n",
    "    cellLine_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "    for file in os.listdir(cellLine_dir):\n",
    "        if file.count(\"annotated.gtf\")==1:\n",
    "            assembly_gtf = cellLine_dir + \"/\" + file\n",
    "            get_new_intron_bed(assembly_gtf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "assemblyGTF = work_dir + \"/stringtie_res5/HCT116_NCM460/sgRNA_HCT116_NCM460.minCoverage_5.gap_20.gtf\"\n",
    "get_new_intron_bed(assemblyGTF)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new transcripts: intron retained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "hg38_intron_bed = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.intron.bed\"\n",
    "hg38_intron_bed_modif = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.intron.modif.bed\"\n",
    "\n",
    "# # modif hg38 intron bed \n",
    "awk '{OFS=\"\\t\"} {print $2,$3,$4}' gencode.v37.chr_patch_hapl_scaff.annotation.intron.bed | \\\n",
    "awk -F \"|\" '{OFS=\"\\t\"} {print $3,$1}' >  gencode.v37.chr_patch_hapl_scaff.annotation.intron.modif.bed\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_intron_retained_bed(assembly_gtf): \n",
    "    exon_bed = assembly_gtf[:-3] + \"exon.modif.bed\"\n",
    "    exon_bed_ref = assembly_gtf[:-3] + \"exon.modif.ref.bed\"\n",
    "    intron_retained_bed = assembly_gtf[:-3] + \"intron_retained.bed\"\n",
    "    cmd = \"awk '{OFS=\\\"\\t\\\"} $3==\\\"exon\\\" {print $4,$5,$10}' %s |awk -F \\\"\\\\\\\"\\\" '{OFS=\\\"\\t\\\"} {print $2,$1}' > %s\" % (\n",
    "        assembly_gtf, exon_bed)\n",
    "    os.system(cmd)\n",
    "    basename = os.path.basename(assembly_gtf)\n",
    "    tmap = assembly_gtf.split(\"annotated.gtf\")[0] + basename.split(\"annotated.gtf\")[0] + \"gtf.tmap\"\n",
    "    if os.path.exists(tmap):\n",
    "        tmap_df = pd.read_csv(tmap, sep=\"\\t\")\n",
    "        tmap_df = tmap_df.loc[:, [\"ref_id\", \"qry_id\"]]\n",
    "        exon_df = pd.read_csv(exon_bed, sep=\"\\t\", header=None)\n",
    "        exon_df = exon_df.iloc[:, 0:3]\n",
    "        exon_df.columns = [\"qry_id\", \"start\", \"end\"]\n",
    "        merge = pd.merge(exon_df, tmap_df, on=\"qry_id\")\n",
    "        merge = merge.loc[:, [\"ref_id\", \"start\", \"end\", \"qry_id\"]]\n",
    "        merge.to_csv(exon_bed_ref, index=False, sep=\"\\t\", header=False)\n",
    "        cmd = \"/NAS/wg_looking/Tools/bedtools2/bin/bedtools intersect \\\n",
    "               -a {exon_bed_ref} \\\n",
    "               -b {hg38_intron_bed} -F 0.01 > \\\n",
    "               {intron_retained_bed}\".format(\n",
    "            exon_bed_ref=exon_bed_ref, \n",
    "            hg38_intron_bed=hg38_intron_bed_modif, \n",
    "            intron_retained_bed=intron_retained_bed)\n",
    "        os.system(cmd)\n",
    "\n",
    "    \n",
    "\n",
    "for CellLine in [\"HCT116\", \"K562\", \"MCF10A\", \"MDAMB231\", \"NCM460\"]:\n",
    "    cellLine_dir = work_dir + \"/stringtie_res5/\" + CellLine\n",
    "    for file in os.listdir(cellLine_dir):\n",
    "        if file.count(\"annotated.gtf\")==1:\n",
    "            assembly_gtf = cellLine_dir + \"/\" + file\n",
    "            get_intron_retained_bed(assembly_gtf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gffcompare sgRNA with totalRNA gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for K562\n",
    "outDir = work_dir + \"/stringtie_res5/K562\"\n",
    "sgK562 = work_dir + \"/stringtie_res5/K562/sgRNA_K562.minCoverage_5.gap_20.gtf\"\n",
    "totalK562 = work_dir + \"/stringtie_res5/K562/totalRNA_K562.minCoverage_5.gap_20.gtf\"\n",
    "totalK562_1 = work_dir + \"/stringtie_res5/K562/totalRNA_K562_1.minCoverage_5.gap_20.gtf\"\n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=totalK562_1, outprefix=outDir + \"/sg_vs_total_K562\", input=sgK562)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "# for HCT116\n",
    "outDir = work_dir + \"/stringtie_res5/HCT116\"\n",
    "sgHCT116 = work_dir + \"/stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "totalHCT116 = work_dir + \"/stringtie_res5/HCT116/totalRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=totalHCT116, outprefix=outDir + \"/sg_vs_total_HCT116\", input=sgHCT116)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "# for NCM460\n",
    "outDir = work_dir + \"/stringtie_res5/NCM460\"\n",
    "sgNCM460 = work_dir + \"/stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\"\n",
    "totalNCM460 = work_dir + \"/stringtie_res5/NCM460/totalRNA_NCM460.minCoverage_5.gap_20.gtf\"    \n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=totalNCM460, outprefix=outDir + \"/sg_vs_total_NCM460\", input=sgNCM460)\n",
    "os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gffcompare: Cancer vs Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for HCT116 vs NCM460\n",
    "outDir = work_dir + \"/stringtie_res5\"\n",
    "sgHCT116 = work_dir + \"/stringtie_res5/HCT116/sgRNA_HCT116.minCoverage_5.gap_20.gtf\"\n",
    "sgNCM460 = work_dir + \"/stringtie_res5/NCM460/sgRNA_NCM460.minCoverage_5.gap_20.gtf\" \n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=sgNCM460, outprefix=outDir + \"/sgHCT116_vs_sgNCM460\", input=sgHCT116)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "\n",
    "# for MDAMB231 vs MCF10A\n",
    "sgMDAMB231 = work_dir + \"/stringtie_res5/MDAMB231/sgRNA_MDAMB231.minCoverage_5.gap_20.gtf\"\n",
    "sgMCF10A = work_dir + \"/stringtie_res5/MCF10A/sgRNA_MCF10A.minCoverage_5.gap_20.gtf\"\n",
    "cmd = \"{gffcompare} -r {ref_gtf} -o {outprefix}  {input} \".format(\n",
    "        gffcompare=gffcompare, ref_gtf=sgMCF10A, outprefix=outDir + \"/sgMDAMB231_vs_sgMCF10A\", input=sgMDAMB231)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "toc-hr-collapsed": true
   },
   "source": [
    "## transcripts orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orientation\n",
    "newFa = work_dir + \"/stat/sgRNA_new_transcripts.fa\"\n",
    "fastq = work_dir + \"/Saturation_analysis/sgRNA.fastq\"\n",
    "bam = work_dir + \"/sgRNA_new_transcripts/sgRNA_new_transcripts.sort.bam\"\n",
    "minimap2_run(newFa, fastq, bam)\n",
    "\n",
    "\"/NAS/wg_looking/Tools/samtools-1.11/samtools view sgRNA_new_transcripts.sort.bam |grep TCGTATGCTGCTGATGCTCGT  |awk '{print $3}' |sort -u |wc -l\"\n",
    "\"/NAS/wg_looking/Tools/samtools-1.11/samtools view sgRNA_new_transcripts.sort.bam |awk '{print $2}' |sort |uniq -c\"\n",
    "\"/NAS/wg_looking/Tools/samtools-1.11/samtools view sgRNA_new_transcripts.sort.bam | awk '$2==0 || $2==16 {print $0}'> sgRNA_new_transcripts.sort.filter.sam\"\n",
    "\"grep TCGTATGCTGCTGATGCTCGT sgRNA_new_transcripts.sort.filter.sam| awk '$2==0 {print $3}' |sort -u > positive_strand_transcripts.1.txt \"\n",
    "\"grep TCGTATGCTGCTGATGCTCGT sgRNA_new_transcripts.sort.filter.sam| awk '$2==16 {print $3}' |sort -u > negative_strand_transcripts.1.txt \"\n",
    "\"grep CATTGCGCAATCTGTCTCT sgRNA_new_transcripts.sort.filter.sam| awk '$2==16 {print $3}' |sort -u > positive_strand_transcripts.2.txt \"\n",
    "\"grep CATTGCGCAATCTGTCTCT sgRNA_new_transcripts.sort.filter.sam| awk '$2==0 {print $3}' |sort -u > negative_strand_transcripts.2.txt \"\n",
    "\"sort positive_strand_transcripts.1.txt positive_strand_transcripts.2.txt | uniq -d > positive_strand_transcripts.txt\"\n",
    "\"sort negative_strand_transcripts.1.txt negative_strand_transcripts.2.txt | uniq -d > negative_strand_transcripts.txt\"\n",
    "\"sort positive_strand_transcripts.txt negative_strand_transcripts.txt | uniq -d |wc -l\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quntification: prepDE.py3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "ref_gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "prepDE = \"/NAS/wg_looking/gRNA_ONT/code/prepDE.py3\"\n",
    "stringtie = \"/NAS/wg_looking/Tools/stringtie-2.1.7.Linux_x86_64/stringtie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ID in list(fastq_dict.keys()):\n",
    "    CellLine = ID.split(\"_\")[1]\n",
    "    name = ID[:-2]\n",
    "    if ID == \"totalRNA_K562_1\" or len(ID.split(\"_\"))==2:\n",
    "        name = ID\n",
    "    bam = work_dir + \"/%s/%s.Q7.L300.sort.bam\" % (ID, ID)\n",
    "    gtf = work_dir + \"/stringtie_res5/%s/%s.minCoverage_5.gap_20.gtf\" % (CellLine, name)\n",
    "    out_gtf = work_dir + \"/stat/%s.for_prepDE.gtf\" % (ID)\n",
    "    if not os.path.exists(out_gtf):\n",
    "        cmd = \"{stringtie} -L -p 50 -e -c 5 -s 5 -G {gtf} -g 20 -o {outgtf} {bam}\".format(\n",
    "                stringtie=stringtie, gtf=gtf, outgtf=out_gtf, bam=bam)\n",
    "        os.system(cmd)\n",
    "    gtf_list = work_dir + \"/%s/%s.gtf.list\" % (ID, ID)\n",
    "    with open(gtf_list, \"w\") as out:\n",
    "            out.write(\"%s\\t%s\\n\" % (ID, out_gtf))\n",
    "            out.flush()\n",
    "    g_count = work_dir + \"/%s/prepDE_assembly_gtf_gene_count.csv\" % ID\n",
    "    t_count = work_dir + \"/%s/prepDE_assembly_gtf_transcript_count.csv\" % ID\n",
    "    os.system('{prepDE} -i {gtf_list} -g {g_count} -t {t_count} -l 1000'.format(\n",
    "        prepDE=prepDE, gtf_list=gtf_list, g_count=g_count, t_count=t_count))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assembly: Trinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run Genome-guided Trinity \n",
    "\n",
    "/NAS/wg_looking/Tools/trinityrnaseq-v2.13.2/Trinity --genome_guided_bam sgRNA_MCF10A_1.Q7.L300.sort.bam \\\n",
    "     --genome_guided_max_intron 100000 \\\n",
    "     --max_memory 10G --CPU 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "work_dir = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "ID = 'sgRNA_K562_1'\n",
    "\n",
    "# Trinity\n",
    "/NAS/wg_looking/Tools/trinityrnaseq-v2.13.2/Trinity \\\n",
    "--seqType fq \\\n",
    "--single  /NAS/wg_looking/gRNA_ONT/QC/sgRNA_K562_1.trimming.Q7.L300.fq.gz \\\n",
    "--max_memory 50G \\\n",
    "--CPU 6 \n",
    "\n",
    "\n",
    "\n",
    "# mapping\n",
    "def minimap2_run(ref, fq, bam):\n",
    "    sam = bam.split(\"sort.bam\")[0] + \"sam\"\n",
    "    cmd = \"{minimap2} -ax splice --MD {ref} {fq} > {sam}\".format(minimap2=minimap2, ref=ref, fq=fq, sam=sam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} sort -@ 100 {sam} -o {bam}\".format(\n",
    "        samtools=samtools, sam=sam, bam=bam)\n",
    "    os.system(cmd)\n",
    "    cmd = \"{samtools} index {bam}\".format(\n",
    "        samtools=samtools, bam=bam)\n",
    "    os.system(cmd)\n",
    "    \n",
    "\n",
    "\n",
    "fa = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/trinity_out_dir.Trinity.fasta'\n",
    "bam = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.Trinity.sort.bam'\n",
    "minimap2_run(hg38, fa, bam)\n",
    "\n",
    "\n",
    "# call variants\n",
    "os.chdir('/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1')\n",
    "outVCF = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.Trinity.vcf'\n",
    "cmd = \"{cuteSV} --threads 50 \\\n",
    "        --report_readid \\\n",
    "        --min_read_len 300 \\\n",
    "        --max_split_parts -1 \\\n",
    "        --merge_ins_threshold 100 \\\n",
    "        --min_support 1 \\\n",
    "        --genotype \\\n",
    "        --min_size 300 \\\n",
    "        --diff_ratio_merging_INS 0.3 \\\n",
    "        --max_cluster_bias_INS 100 \\\n",
    "        --max_cluster_bias_DEL 100 \\\n",
    "        --diff_ratio_merging_DEL 0.3 \\\n",
    "        --sample {sampleID} \\\n",
    "        {sort_bam} {reference} {outVCF} {work_dir}\".format(\n",
    "    cuteSV=cuteSV, sampleID='sgRNA_K562_1', sort_bam=bam, reference=hg38, outVCF=outVCF, work_dir=\"./\")\n",
    "os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqkit = \"/NAS/wg_looking/Tools/seqkit\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "#                         extract fa                        #\n",
    "#############################################################\n",
    "def extractFafromVCF(vcf, vcf_fa):\n",
    "    with open(vcf_fa, \"w\") as fout:\n",
    "        with open(vcf, \"r\") as fin:\n",
    "            for line in fin:\n",
    "                if line[0] != \"#\":\n",
    "                    info = line.split()\n",
    "                    detail = info[7].split(\";\")\n",
    "                    detail_dict = {}\n",
    "                    for d in detail[1:]:\n",
    "                        detail_dict[d.split(\"=\")[0]] = d.split(\"=\")[1]\n",
    "                    if detail_dict['SVTYPE'] == \"INS\":\n",
    "                        seq = info[4]\n",
    "                        out = \">%s:%s-%s\\n%s\\n\" % (info[0], info[1], info[2], seq)\n",
    "                        fout.write(out)\n",
    "                        fout.flush()\n",
    "\n",
    "\n",
    "\n",
    "outVCF = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.Trinity.vcf'\n",
    "vcf_fa = outVCF + \".fa\"\n",
    "extractFafromVCF(outVCF, vcf_fa)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                         repeatmasker                          #\n",
    "#################################################################\n",
    "\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "RepeatMasker = \"/NAS/wg_looking/Tools/RepeatMasker/RepeatMasker\"\n",
    "\n",
    "def RM_run(clean_fa, outDir):\n",
    "    cmd = \"{RepeatMasker} -pa 100 -a -species human {fa} -dir {outDir}\".format(\n",
    "        RepeatMasker=RepeatMasker, fa=clean_fa, outDir=outDir)\n",
    "    os.system(cmd)  # 可以使用-div 10，进一步保证重复序列的正确性.则结果会是div<10\n",
    "\n",
    "\n",
    "\n",
    "outVCF = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.Trinity.vcf'\n",
    "vcf_fa = outVCF + \".fa\"\n",
    "outDir = '/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1'\n",
    "RM_run(vcf_fa, outDir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract gene function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pyquery import PyQuery as pq\n",
    "from requests.exceptions import ReadTimeout, ConnectionError, RequestException\n",
    "import sys\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "\n",
    "def NCBI_gene_summary(EntrezID):\n",
    "    Summary = \"\"\n",
    "    if EntrezID != \"NA\":\n",
    "        try:\n",
    "            http = \"https://www.ncbi.nlm.nih.gov/gene/\" + EntrezID\n",
    "            data = requests.get(http).content\n",
    "            doc = pq(data)\n",
    "            summary = doc('#summaryDl')\n",
    "            items = summary.text()\n",
    "            list_items = items.split(\"\\n\")\n",
    "            if \"Summary\" in list_items:\n",
    "                p = list_items.index(\"Summary\")\n",
    "                Summary = list_items[p + 1]\n",
    "            elif \"Expression\" in list_items:\n",
    "                p = list_items.index(\"Expression\")\n",
    "                Summary = list_items[p + 1]\n",
    "        except ReadTimeout:\n",
    "            Summary = \"timeout\"\n",
    "        except ConnectionError:\n",
    "            Summary = \"connection Error\"\n",
    "        except RequestException:\n",
    "            Summary = \"Error\"\n",
    "    else:\n",
    "        Summary = \"NA\"\n",
    "    return(Summary)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Entrez_ID = \"/NAS/wg_looking/gRNA_ONT/database/ENSEMBL_ID_Entrez_ID_all.txt\"\n",
    "gene_summary_file = \"/NAS/wg_looking/gRNA_ONT/database/ENSEMBL_ID_Entrez_ID_summary.txt\"\n",
    "# lab6 in small white floor\n",
    "# Entrez_ID = \"/home/looking/ENSEMBL_ID_Entrez_ID_all.txt\"\n",
    "# gene_summary_file = \"/home/looking/ENSEMBL_ID_Entrez_ID_summary.txt\"\n",
    "# gene_file = sys.argv[1]\n",
    "# gene_summary_file = sys.argv[2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########\n",
    "Entrez_ID_dict = {}\n",
    "with open(Entrez_ID, \"r\") as f:\n",
    "    for line in f:\n",
    "        info = line.split()\n",
    "        if info[0] == \"gene_id\":\n",
    "            continue\n",
    "        Entrez_ID_dict[info[0]] = info[1]\n",
    "\n",
    "        \n",
    "## get  summary\n",
    "pool = Pool(15)\n",
    "# summary_dict = {}\n",
    "for ensymblID in list(Entrez_ID_dict.keys())[56:]:\n",
    "    EntrezID = Entrez_ID_dict[ensymblID]\n",
    "    res = pool.apply_async(NCBI_gene_summary, args=(EntrezID,))\n",
    "    summary_dict[ensymblID] = res.get()\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "## get  summary\n",
    "pool = Pool(10)\n",
    "# summary_dict = {}\n",
    "for ensymblID in list(Entrez_ID_dict.keys())[13092:]:\n",
    "    EntrezID = Entrez_ID_dict[ensymblID]\n",
    "    res = pool.apply_async(NCBI_gene_summary, args=(EntrezID,))\n",
    "    summary_dict[ensymblID] = res.get()\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n",
    "len(summary_dict.keys()) # 13115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dict = {}\n",
    "with open(gene_summary_file, \"r\") as fin:\n",
    "    for line in fin:\n",
    "        info = line.split()\n",
    "        summary_dict[info[0]] = info[1]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(gene_summary_file, 'a') as f_1:\n",
    "        for ensymblID in list(Entrez_ID_dict.keys())[12448:]:\n",
    "                Entrez = Entrez_ID_dict[ensymblID]\n",
    "                if Entrez != \"NA\":\n",
    "                    try:\n",
    "                        http = \"https://www.ncbi.nlm.nih.gov/gene/\" + Entrez\n",
    "                        data = requests.get(http).content\n",
    "                        doc = pq(data)\n",
    "                        summary = doc('#summaryDl')\n",
    "                        items = summary.text()\n",
    "                        list_items = items.split(\"\\n\")\n",
    "                        if \"Summary\" in list_items:\n",
    "                            p = list_items.index(\"Summary\")\n",
    "                            f_1.write(ensymblID + \"\\t\" + list_items[p + 1] + \"\\n\")\n",
    "                            f_1.flush()\n",
    "                        elif \"Expression\" in list_items:\n",
    "                            p = list_items.index(\"Expression\")\n",
    "                            f_1.write(ensymblID + \"\\t\" + list_items[p + 1] + \"\\n\")\n",
    "                    except ReadTimeout:\n",
    "                        f_1.write(ensymblID + \"\\t\" + \"timeout\" + \"\\n\")\n",
    "                    except ConnectionError:\n",
    "                        f_1.write(ensymblID + \"\\t\" + \"connection Error\" + \"\\n\")\n",
    "                    except RequestException:\n",
    "                        f_1.write(ensymblID + \"\\t\" + \"error\" + \"\\n\")\n",
    "\n",
    "                        \n",
    "list(Entrez_ID_dict.keys()).index(ensymblID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gffread = \"/NAS/wg_looking/Tools/gffread-0.12.7.Linux_x86_64/gffread\"\n",
    "bedtools = \"/NAS/wg_looking/Tools/bedtools2/bin/bedtools\"\n",
    "gtf = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf\"\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "work_dir  = \"/NAS/wg_looking/gRNA_ONT\"\n",
    "\n",
    "\n",
    "# prepare transcript fa without intron\n",
    "ref_transcript_fa = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.fa\"\n",
    "cmd = \"{gffread} -w {outFa} -g {refFa} {gtf}\".format(\n",
    "    gffread=gffread, outFa=ref_transcript_fa, refFa=hg38, gtf=gtf)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "# prepare transcript fa with intron\n",
    "transcript_bed = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.bed\"\n",
    "ref_transcript_fa_with_intron = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.with_intron.fa\"\n",
    "cmd = \"{bedtools} getfasta -fi {genome_fasta} -bed {input_bed} -fo {outFa} -nameOnly\".format(\n",
    "    bedtools=bedtools, genome_fasta=hg38, input_bed=transcript_bed, outFa=ref_transcript_fa_with_intron)\n",
    "os.system(cmd)\n",
    "RM_run(ref_transcript_fa_with_intron, \"/NAS/wg_looking/gRNA_ONT/database\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_transcript_fa_with_intron = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.with_intron.fa\"\n",
    "outDir = work_dir + \"/database/ref_transcript_with_intron/RepeatMasker_res\"\n",
    "splitfa_dir = work_dir + \"/database/ref_transcript_with_intron/split_fa\" \n",
    "\n",
    "for dirs in [outDir, splitfa_dir]:\n",
    "    if not os.path.exists(dirs):\n",
    "        os.makedirs(dirs)\n",
    "cmd = \"split -l 10000 {fa} {dir}/\".format(fa=ref_transcript_fa_with_intron, dir=splitfa_dir)\n",
    "os.system(cmd)\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "os.chdir(outDir)\n",
    "pool = Pool(10)\n",
    "for fa in os.listdir(splitfa_dir):\n",
    "    if os.path.exists(fa + \".out\"):\n",
    "        continue\n",
    "    input_fa = splitfa_dir + \"/\" + fa\n",
    "    pool.apply_async(RM_run, args=(input_fa, outDir,))\n",
    "\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "del pool\n",
    "\n",
    "\n",
    "\n",
    "## merge out\n",
    "os.chdir(outDir)\n",
    "basename = os.path.basename(ref_transcript_fa_with_intron)\n",
    "os.system(\"less -S aa.out |head -n 3 > %s.rm.tmp\" % basename)\n",
    "for file in os.listdir(outDir):\n",
    "    if file.count(\"out\") == 1:\n",
    "        os.system(\"tail -n +4 %s >> %s.rm.tmp\" % (file, basename))\n",
    "os.system(\"mv %s.rm.tmp %s.out\" % (basename, basename))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recognize guide RNA in ref transcript fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "blastDir = \"/NAS/wg_looking/Tools/ncbi-blast-2.11.0+/bin\"\n",
    "transcript_fa = \"/NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.transcript.fa\"\n",
    "guideRNA_Fa = \"/NAS/wg_looking/gRNA_ONT/database/guideRNA.fa\"\n",
    "guideRNA_DB = \"/NAS/wg_looking/gRNA_ONT/database/guideRNA\"\n",
    "\n",
    "\n",
    "# make index for blast\n",
    "cmd = \"/NAS/wg_looking/Tools/ncbi-blast-2.11.0+/bin/makeblastdb -in {fa} -dbtype nucl -parse_seqids -out {db}\".format(\n",
    "      fa=guideRNA_Fa, db=guideRNA_DB)\n",
    "os.system(cmd)\n",
    "\n",
    "# start alignment\n",
    "blast_out = \"/NAS/wg_looking/gRNA_ONT/stat/gencode.v37.chr_patch_hapl_scaff.transcript.fa.guideRNA.out\"\n",
    "cmd = \"{blastDir}/blastn -task blastn-short -word_size 10 -strand plus \\\n",
    "    -db {db} \\\n",
    "    -query {query} \\\n",
    "    -out {blast_out} \\\n",
    "    -outfmt 6 -num_threads 50 -num_alignments 1000000000\".format(\n",
    "        blastDir=blastDir, db=guideRNA_DB, query=transcript_fa, blast_out=blast_out)\n",
    "os.system(cmd)\n",
    "\n",
    "\n",
    "\n",
    "total_k562 = \"/NAS/wg_looking/gRNA_ONT/QC/totalRNA_K562_1.trimming.Q7.L300.fa\"\n",
    "blast_out = \"/NAS/wg_looking/gRNA_ONT/stat/totalRNA_K562_1.trimming.Q7.L300.fa.guideRNA.out\"\n",
    "cmd = \"{blastDir}/blastn -task blastn-short -word_size 10 -strand plus \\\n",
    "    -db {db} \\\n",
    "    -query {query} \\\n",
    "    -out {blast_out} \\\n",
    "    -outfmt 6 -num_threads 50 -num_alignments 1000000000\".format(\n",
    "        blastDir=blastDir, db=guideRNA_DB, query=total_k562, blast_out=blast_out)\n",
    "os.system(cmd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# basecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/NAS/zxjs/Tools/guppy-4.4.1/bin/guppy_basecaller  \\\n",
    "-i /NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/fast5_pass  \\\n",
    "--save_path /NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/basecall_pass  \\\n",
    "-c /NAS/zxjs/Tools/guppy-4.4.1/data/dna_r9.4.1_450bps_hac.cfg --num_callers 4 -x \"cuda:2\" --fast5_out \\\n",
    "--disable_pings 1 --recursive  --qscore_filtering  --min_qscore 7\n",
    "\n",
    "\n",
    "# --trim_barcodes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "/NAS/wg_looking/Tools/Porechop/porechop-runner.py \\\n",
    "-i /NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/basecall_pass/pass \\\n",
    "-b /NAS/wg_tkl/aws_tkl/Transponsable_Element_Project/no_sample/DNA_CH3minus_RNAsmart3_210624/no_sample/20210624_1233_MN27380_FAQ13233_0de2bde3/basecall_pass/pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1HS\n",
    "chr12:87747544-87749422\n",
    "Score = 1000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# genomeview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path\n",
    "# sys.path.append('/home/wg_looking/.local/lib/python3.6/site-packages')\n",
    "import genomeview\n",
    "from genomeview.bamtrack import SingleEndBAMTrack, PairedEndBAMTrack\n",
    "import genomeview.axis\n",
    "import genomeview.graphtrack\n",
    "from genomeview import genomesource\n",
    "\n",
    "\n",
    "hg38 = \"/NAS/wg_looking/human_ref/GRCh38.p13.genome.fa\"\n",
    "# /NAS/wg_looking/Tools/pygtftk-1.5.3/bin/gtftk convert \\\n",
    "# -i /NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.gtf \\\n",
    "# -o /NAS/wg_looking/human_ref/gencode.v37.chr_patch_hapl_scaff.annotation.bed \n",
    "\n",
    "\n",
    "sampleID = \"K562\"\n",
    "track_info = {\"sgRNA_K562\": \"/NAS/wg_looking/gRNA_ONT/sgRNA_K562_1/sgRNA_K562_1.Q7.L300.sort.bam\",\n",
    "              \"totalRNA_K562\": \"/NAS/wg_looking/gRNA_ONT/totalRNA_K562_1/totalRNA_K562_1.Q7.L300.sort.bam\",\n",
    "             \"hg38_annotation\":\"/NAS/wg_looking/human_ref/hgTables.bed\"}\n",
    "\n",
    "# chr14:74,198,541-74,201,676\n",
    "chrom = \"chr14\"\n",
    "start = 74198541\n",
    "end = 74201676\n",
    "\n",
    "tracks = genomeview.visualize_data(track_info, chrom, start, end, hg38)\n",
    "genomeview.save(tracks, \"/NAS/wg_looking/gRNA_ONT/stat/%s_%s_%s_genomeview_ALU_insertion.svg\" % (sampleID, chrom, start))\n",
    "\n",
    "/NAS/wg_looking/Tools/webkitToPDF \\\n",
    "/NAS/wg_looking/gRNA_ONT/stat/%s_%s_%s_genomeview_ALU_insertion.svg \\\n",
    "/NAS/wg_looking/gRNA_ONT/stat/%s_%s_%s_genomeview_ALU_insertion.pdf\n",
    "#\n",
    "track_info = {\"%s_blood\" % sampleID:\"/NAS/wg_tkl/NSCLC_ONT/bam_minimap2/%s__blood_pass.sort.bam\" % sampleID,\n",
    "              \"%s_Cancer\" % sampleID:\"/NAS/wg_tkl/NSCLC_ONT/bam_minimap2/%s_Cancer_cells_pass.sort.bam\" % sampleID}\n",
    "chrom = \"chr1\"\n",
    "start = 820908\n",
    "\n",
    "tracks = genomeview.visualize_data(track_info, chrom, start-1000, start+1000, genome_path)\n",
    "genomeview.save(tracks, \"/NAS/wg_tkl/NSCLC_ONT/bam_ngmlr/%s_%s_genomeview_sniffles.svg\" % (chrom, start))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "068053a104b39fdba82ecf252b0c424dc6628bcb",
   "visible": [
    {
     "cellId": "group",
     "hash": "19d3d3d153f22e318cbe061df9e685ba5165b0aa",
     "reason": "This groups other lint messages",
     "reportId": "group",
     "reportType": "confusenotebook",
     "suggestion": null,
     "text": "Confuse Notebook"
    },
    {
     "cellId": 4,
     "hash": "9c3ef197653381d1bac1908cd7eeabc5a639ba0b",
     "reason": "Empty cells in between executable ones occupy space and might impact the readability of the notebook.",
     "reportId": "c3",
     "reportType": "confusenotebook",
     "suggestion": "Please consider removing it to improve the readability.",
     "text": "Cell 4 is empty in the middle of the notebook"
    },
    {
     "cellId": 5,
     "hash": "e27ed6779cb97dd78414db747573c8805c376775",
     "reason": "A markdown cell at the end of the notebook can conclude it, presenting a summary of the obtained results.",
     "reportId": "c5",
     "reportType": "confusenotebook",
     "suggestion": "Please consider adding a markdown cell to conclude the notebook.",
     "text": "The last cell of the notebook is not a markdown cell"
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
